{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TZidhXIkgzt",
        "outputId": "f5487e40-378f-442b-b478-007810d2851a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bImIuKhaOPUI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcfb5bb3-ea7a-40fa-b398-a5a0fb325f55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.88-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.88-py3-none-any.whl (932 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m932.9/932.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.88 ultralytics-thop-2.0.14\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c 'from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights; maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT)'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3crAd1NY0wy0",
        "outputId": "1e4abdc3-74ee-4976-cc23-ef0b9aac5814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n",
            "100% 170M/170M [00:02<00:00, 85.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7BhBDC6w52M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b074d2f-6e7b-493f-d15b-4e695b03dd28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt to 'yolo11n-pose.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.97M/5.97M [00:00<00:00, 64.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "model = YOLO(\"yolo11n-pose.pt\")\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.models.detection as detection\n",
        "import os\n",
        "import json\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import cv2\n",
        "import time\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.model_selection import KFold\n",
        "from scipy.ndimage import shift\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKtiXroidQs-",
        "outputId": "f399aa9e-3123-449d-d384-4d9c99fc9290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing video  /content/drive/MyDrive/FYP/Dataset/52_1.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/53_3.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/54_2.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/55_1.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/57_3.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/56_2.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/58_1.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/59_3.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/60_2.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/61_1.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/62_3.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/63_2.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/64_1.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/65_3.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/67_1.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/68_3.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/69_1.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/72_1.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/73_3.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/74_2.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/76_1.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/75_3.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/77_2.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/77_3.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/78_2.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/80_3.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/81_2.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/82_1.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/83_3.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/85_2.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/84_1.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/86_3.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/87_2.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/88_1.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/89_3.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/90_2.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/91_1.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/92_3.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/93_2.mp4\n",
            "processing video  /content/drive/MyDrive/FYP/Dataset/94_1.mp4\n",
            "Video processing complete!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# MIDDLE_FRAMES_NO = 120\n",
        "# Define the specific indexes of keypoints to retain\n",
        "specific_indexes = [0, 5, 6, 7, 8, 9, 10]\n",
        "\n",
        "# Function to process videos and store tracked keypoints in JSON files\n",
        "def process_videos(video_dir, json_dir):\n",
        "    # Ensure the JSON directory exists\n",
        "    os.makedirs(json_dir, exist_ok=True)\n",
        "\n",
        "    # Loop through all video files in the provided directory\n",
        "    for video_filename in os.listdir(video_dir):\n",
        "        video_path = os.path.join(video_dir, video_filename)\n",
        "        # Save the tracked keypoints to a JSON file with the same name as the video\n",
        "        json_filename = os.path.splitext(video_filename)[0] + \".json\"\n",
        "        json_path = os.path.join(json_dir, json_filename)\n",
        "\n",
        "        if os.path.exists(json_path):\n",
        "            continue\n",
        "\n",
        "        # Skip non-video files\n",
        "        if not video_filename.lower().endswith(('.mp4', '.mov', '.avi')):\n",
        "            continue\n",
        "\n",
        "        print(\"processing video \", video_path)\n",
        "        # Open the video file\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        vid_tracked_keypoints = {}  # Dictionary to store tracked keypoints per frame\n",
        "\n",
        "        # Get the total number of frames in the video\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Create a new tracker instance for this video\n",
        "        model = YOLO(\"/content/yolo11n-pose.pt\")  # Load the model again for each video\n",
        "\n",
        "        # Loop through the middle 70 frames\n",
        "        for frame_num in range(0, total_frames):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
        "            success, frame = cap.read()\n",
        "\n",
        "            if not success:\n",
        "                break\n",
        "\n",
        "            # Run YOLOv11 tracking on the frame\n",
        "            results = model.track(frame, persist=True, max_det=2, classes=[0], verbose=False, device=\"cpu\")\n",
        "\n",
        "            # Extract keypoints and track IDs from the results\n",
        "            keypoints = results[0].keypoints.data\n",
        "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "\n",
        "            # Initialize dictionary to store the tracked keypoints for this frame\n",
        "            frame_tracked_keypoints = {}\n",
        "\n",
        "            # Ensure there are exactly 2 people detected in the frame\n",
        "            if len(track_ids) != 2:\n",
        "                raise ValueError(f\"Expected 2 persons, but found {len(track_ids)} persons in frame {frame_num}.\")\n",
        "\n",
        "            # Iterate over each person's keypoints\n",
        "            for person_data, track in zip(keypoints, track_ids):\n",
        "                person_keypoints = []\n",
        "                for index in specific_indexes:\n",
        "                    x, y, conf = person_data[index].tolist()\n",
        "                    # if conf >= 0.25:\n",
        "                    person_keypoints.append([x, y])  # Add valid (x, y) points\n",
        "                    # else:\n",
        "                    #     person_keypoints.append([0, 0])  # Add (0, 0) for low confidence keypoints\n",
        "\n",
        "                # Store the tracked keypoints for this person\n",
        "                frame_tracked_keypoints[track] = person_keypoints\n",
        "\n",
        "            # Store the tracked keypoints for this frame\n",
        "            vid_tracked_keypoints[frame_num] = frame_tracked_keypoints\n",
        "\n",
        "        with open(json_path, 'w') as json_file:\n",
        "            json.dump(vid_tracked_keypoints, json_file, indent=4)\n",
        "\n",
        "        # Release the video capture object\n",
        "        cap.release()\n",
        "\n",
        "    print(\"Video processing complete!\")\n",
        "\n",
        "# Example usage\n",
        "video_dir = '/content/drive/MyDrive/FYP/Dataset'  # Replace with the path where videos are stored\n",
        "json_dir = '/content/drive/MyDrive/FYP/keypoints'  # Replace with the path where JSON files will be saved\n",
        "process_videos(video_dir, json_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0hO7w1XJIhW",
        "outputId": "fd43552d-df72-458c-ab48-294ad4535526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          action                                  keypoints_path  \\\n",
            "video_id                                                           \n",
            "1_2            1   /content/drive/MyDrive/FYP/keypoints/1_2.json   \n",
            "2_2            1   /content/drive/MyDrive/FYP/keypoints/2_2.json   \n",
            "3_2            1   /content/drive/MyDrive/FYP/keypoints/3_2.json   \n",
            "4_2            1   /content/drive/MyDrive/FYP/keypoints/4_2.json   \n",
            "16_1           0  /content/drive/MyDrive/FYP/keypoints/16_1.json   \n",
            "...          ...                                             ...   \n",
            "90_2           1  /content/drive/MyDrive/FYP/keypoints/90_2.json   \n",
            "91_1           0  /content/drive/MyDrive/FYP/keypoints/91_1.json   \n",
            "92_3           2  /content/drive/MyDrive/FYP/keypoints/92_3.json   \n",
            "93_2           1  /content/drive/MyDrive/FYP/keypoints/93_2.json   \n",
            "94_1           0  /content/drive/MyDrive/FYP/keypoints/94_1.json   \n",
            "\n",
            "                                    silhouettes_path  \\\n",
            "video_id                                               \n",
            "1_2        /content/drive/MyDrive/FYP/Silhouetes/1_2   \n",
            "2_2        /content/drive/MyDrive/FYP/Silhouetes/2_2   \n",
            "3_2        /content/drive/MyDrive/FYP/Silhouetes/3_2   \n",
            "4_2        /content/drive/MyDrive/FYP/Silhouetes/4_2   \n",
            "16_1      /content/drive/MyDrive/FYP/Silhouetes/16_1   \n",
            "...                                              ...   \n",
            "90_2      /content/drive/MyDrive/FYP/Silhouetes/90_2   \n",
            "91_1      /content/drive/MyDrive/FYP/Silhouetes/91_1   \n",
            "92_3      /content/drive/MyDrive/FYP/Silhouetes/92_3   \n",
            "93_2      /content/drive/MyDrive/FYP/Silhouetes/93_2   \n",
            "94_1      /content/drive/MyDrive/FYP/Silhouetes/94_1   \n",
            "\n",
            "                                           video_path  \\\n",
            "video_id                                                \n",
            "1_2        /content/drive/MyDrive/FYP/Dataset/1_2.MOV   \n",
            "2_2        /content/drive/MyDrive/FYP/Dataset/2_2.MOV   \n",
            "3_2        /content/drive/MyDrive/FYP/Dataset/3_2.MOV   \n",
            "4_2        /content/drive/MyDrive/FYP/Dataset/4_2.MOV   \n",
            "16_1      /content/drive/MyDrive/FYP/Dataset/16_1.MOV   \n",
            "...                                               ...   \n",
            "90_2      /content/drive/MyDrive/FYP/Dataset/90_2.mp4   \n",
            "91_1      /content/drive/MyDrive/FYP/Dataset/91_1.mp4   \n",
            "92_3      /content/drive/MyDrive/FYP/Dataset/92_3.mp4   \n",
            "93_2      /content/drive/MyDrive/FYP/Dataset/93_2.mp4   \n",
            "94_1      /content/drive/MyDrive/FYP/Dataset/94_1.mp4   \n",
            "\n",
            "                                          hof_path  \\\n",
            "video_id                                             \n",
            "1_2        /content/drive/MyDrive/FYP/HOF/1_2.json   \n",
            "2_2        /content/drive/MyDrive/FYP/HOF/2_2.json   \n",
            "3_2        /content/drive/MyDrive/FYP/HOF/3_2.json   \n",
            "4_2        /content/drive/MyDrive/FYP/HOF/4_2.json   \n",
            "16_1      /content/drive/MyDrive/FYP/HOF/16_1.json   \n",
            "...                                            ...   \n",
            "90_2      /content/drive/MyDrive/FYP/HOF/90_2.json   \n",
            "91_1      /content/drive/MyDrive/FYP/HOF/91_1.json   \n",
            "92_3      /content/drive/MyDrive/FYP/HOF/92_3.json   \n",
            "93_2      /content/drive/MyDrive/FYP/HOF/93_2.json   \n",
            "94_1      /content/drive/MyDrive/FYP/HOF/94_1.json   \n",
            "\n",
            "                                          dist_path  \\\n",
            "video_id                                              \n",
            "1_2        /content/drive/MyDrive/FYP/Dist/1_2.json   \n",
            "2_2        /content/drive/MyDrive/FYP/Dist/2_2.json   \n",
            "3_2        /content/drive/MyDrive/FYP/Dist/3_2.json   \n",
            "4_2        /content/drive/MyDrive/FYP/Dist/4_2.json   \n",
            "16_1      /content/drive/MyDrive/FYP/Dist/16_1.json   \n",
            "...                                             ...   \n",
            "90_2      /content/drive/MyDrive/FYP/Dist/90_2.json   \n",
            "91_1      /content/drive/MyDrive/FYP/Dist/91_1.json   \n",
            "92_3      /content/drive/MyDrive/FYP/Dist/92_3.json   \n",
            "93_2      /content/drive/MyDrive/FYP/Dist/93_2.json   \n",
            "94_1      /content/drive/MyDrive/FYP/Dist/94_1.json   \n",
            "\n",
            "                                          angle_path  \\\n",
            "video_id                                               \n",
            "1_2        /content/drive/MyDrive/FYP/Angle/1_2.json   \n",
            "2_2        /content/drive/MyDrive/FYP/Angle/2_2.json   \n",
            "3_2        /content/drive/MyDrive/FYP/Angle/3_2.json   \n",
            "4_2        /content/drive/MyDrive/FYP/Angle/4_2.json   \n",
            "16_1      /content/drive/MyDrive/FYP/Angle/16_1.json   \n",
            "...                                              ...   \n",
            "90_2      /content/drive/MyDrive/FYP/Angle/90_2.json   \n",
            "91_1      /content/drive/MyDrive/FYP/Angle/91_1.json   \n",
            "92_3      /content/drive/MyDrive/FYP/Angle/92_3.json   \n",
            "93_2      /content/drive/MyDrive/FYP/Angle/93_2.json   \n",
            "94_1      /content/drive/MyDrive/FYP/Angle/94_1.json   \n",
            "\n",
            "                                          ltp_path  \\\n",
            "video_id                                             \n",
            "1_2        /content/drive/MyDrive/FYP/LTP/1_2.json   \n",
            "2_2        /content/drive/MyDrive/FYP/LTP/2_2.json   \n",
            "3_2        /content/drive/MyDrive/FYP/LTP/3_2.json   \n",
            "4_2        /content/drive/MyDrive/FYP/LTP/4_2.json   \n",
            "16_1      /content/drive/MyDrive/FYP/LTP/16_1.json   \n",
            "...                                            ...   \n",
            "90_2      /content/drive/MyDrive/FYP/LTP/90_2.json   \n",
            "91_1      /content/drive/MyDrive/FYP/LTP/91_1.json   \n",
            "92_3      /content/drive/MyDrive/FYP/LTP/92_3.json   \n",
            "93_2      /content/drive/MyDrive/FYP/LTP/93_2.json   \n",
            "94_1      /content/drive/MyDrive/FYP/LTP/94_1.json   \n",
            "\n",
            "                                               vel_path  \\\n",
            "video_id                                                  \n",
            "1_2        /content/drive/MyDrive/FYP/Velocity/1_2.json   \n",
            "2_2        /content/drive/MyDrive/FYP/Velocity/2_2.json   \n",
            "3_2        /content/drive/MyDrive/FYP/Velocity/3_2.json   \n",
            "4_2        /content/drive/MyDrive/FYP/Velocity/4_2.json   \n",
            "16_1      /content/drive/MyDrive/FYP/Velocity/16_1.json   \n",
            "...                                                 ...   \n",
            "90_2      /content/drive/MyDrive/FYP/Velocity/90_2.json   \n",
            "91_1      /content/drive/MyDrive/FYP/Velocity/91_1.json   \n",
            "92_3      /content/drive/MyDrive/FYP/Velocity/92_3.json   \n",
            "93_2      /content/drive/MyDrive/FYP/Velocity/93_2.json   \n",
            "94_1      /content/drive/MyDrive/FYP/Velocity/94_1.json   \n",
            "\n",
            "                                             fixed_vel_path  \\\n",
            "video_id                                                      \n",
            "1_2       /content/drive/MyDrive/FYP/Fixed_velocity/1_2....   \n",
            "2_2       /content/drive/MyDrive/FYP/Fixed_velocity/2_2....   \n",
            "3_2       /content/drive/MyDrive/FYP/Fixed_velocity/3_2....   \n",
            "4_2       /content/drive/MyDrive/FYP/Fixed_velocity/4_2....   \n",
            "16_1      /content/drive/MyDrive/FYP/Fixed_velocity/16_1...   \n",
            "...                                                     ...   \n",
            "90_2      /content/drive/MyDrive/FYP/Fixed_velocity/90_2...   \n",
            "91_1      /content/drive/MyDrive/FYP/Fixed_velocity/91_1...   \n",
            "92_3      /content/drive/MyDrive/FYP/Fixed_velocity/92_3...   \n",
            "93_2      /content/drive/MyDrive/FYP/Fixed_velocity/93_2...   \n",
            "94_1      /content/drive/MyDrive/FYP/Fixed_velocity/94_1...   \n",
            "\n",
            "                                          fixed_hof_path  fold  \n",
            "video_id                                                        \n",
            "1_2        /content/drive/MyDrive/FYP/Fixed_HOF/1_2.json     0  \n",
            "2_2        /content/drive/MyDrive/FYP/Fixed_HOF/2_2.json     2  \n",
            "3_2        /content/drive/MyDrive/FYP/Fixed_HOF/3_2.json     2  \n",
            "4_2        /content/drive/MyDrive/FYP/Fixed_HOF/4_2.json     1  \n",
            "16_1      /content/drive/MyDrive/FYP/Fixed_HOF/16_1.json     0  \n",
            "...                                                  ...   ...  \n",
            "90_2      /content/drive/MyDrive/FYP/Fixed_HOF/90_2.json     2  \n",
            "91_1      /content/drive/MyDrive/FYP/Fixed_HOF/91_1.json     0  \n",
            "92_3      /content/drive/MyDrive/FYP/Fixed_HOF/92_3.json     1  \n",
            "93_2      /content/drive/MyDrive/FYP/Fixed_HOF/93_2.json     2  \n",
            "94_1      /content/drive/MyDrive/FYP/Fixed_HOF/94_1.json     1  \n",
            "\n",
            "[90 rows x 12 columns]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Function to get ground truth data\n",
        "\n",
        "# Data directories\n",
        "DATA_DIR = '/content/drive/MyDrive/FYP/Dataset'\n",
        "KEYPOINTS_DIR = '/content/drive/MyDrive/FYP/keypoints'\n",
        "SILHOUETES_DIR = '/content/drive/MyDrive/FYP/Silhouetes'\n",
        "HOF_DIR = '/content/drive/MyDrive/FYP/HOF'\n",
        "FIXED_HOF_DIR = '/content/drive/MyDrive/FYP/Fixed_HOF'\n",
        "DIST_DIR = '/content/drive/MyDrive/FYP/Dist'\n",
        "ANGLE_DIR = '/content/drive/MyDrive/FYP/Angle'\n",
        "LTP_DIR = '/content/drive/MyDrive/FYP/LTP'\n",
        "VELOCITY_DIR = '/content/drive/MyDrive/FYP/Velocity'\n",
        "FIXED_VELOCITY_DIR = '/content/drive/MyDrive/FYP/Fixed_velocity'\n",
        "\n",
        "os.makedirs(KEYPOINTS_DIR, exist_ok=True)\n",
        "os.makedirs(SILHOUETES_DIR, exist_ok=True)\n",
        "os.makedirs(HOF_DIR, exist_ok=True)\n",
        "os.makedirs(FIXED_HOF_DIR, exist_ok=True)\n",
        "os.makedirs(DIST_DIR, exist_ok=True)\n",
        "os.makedirs(ANGLE_DIR, exist_ok=True)\n",
        "os.makedirs(LTP_DIR, exist_ok=True)\n",
        "os.makedirs(VELOCITY_DIR, exist_ok=True)\n",
        "os.makedirs(FIXED_VELOCITY_DIR, exist_ok=True)\n",
        "\n",
        "NUM_FOLDS = 3\n",
        "\n",
        "# Function to get ground truth data\n",
        "def get_ground_truth(num_folds=NUM_FOLDS, random_state=42):\n",
        "    video_path_lst, video_lst, action_lst, keypoints_lst, silhouettes_lst, hof_lst, dist_lst, angle_lst = [], [], [], [], [], [], [], []\n",
        "\n",
        "    ltp_lst, vel_lst, fixed_vel_lst, fixed_hof_lst = [], [], [], []\n",
        "\n",
        "    for video in os.listdir(DATA_DIR):\n",
        "        video_name, _ = os.path.splitext(video)\n",
        "        video_id, action = video_name.split('_')\n",
        "\n",
        "        keypoints_path = os.path.join(KEYPOINTS_DIR, f'{video_name}.json')\n",
        "        hof_path = os.path.join(HOF_DIR, f'{video_name}.json')\n",
        "        dist_path = os.path.join(DIST_DIR, f'{video_name}.json')\n",
        "        angle_path = os.path.join(ANGLE_DIR, f'{video_name}.json')\n",
        "        ltp_path = os.path.join(LTP_DIR, f'{video_name}.json')\n",
        "        vel_path = os.path.join(VELOCITY_DIR, f'{video_name}.json')\n",
        "        fixed_vel_path = os.path.join(FIXED_VELOCITY_DIR, f'{video_name}.json')\n",
        "        fixed_hof_path = os.path.join(FIXED_HOF_DIR, f'{video_name}.json')\n",
        "\n",
        "        silhouettes_path = os.path.join(SILHOUETES_DIR, video_name)\n",
        "        video_path = os.path.join(DATA_DIR, video)\n",
        "\n",
        "        video_lst.append(video_name)\n",
        "        action_lst.append(int(action) - 1)\n",
        "        keypoints_lst.append(keypoints_path)\n",
        "        silhouettes_lst.append(silhouettes_path)\n",
        "        video_path_lst.append(video_path)\n",
        "        hof_lst.append(hof_path)\n",
        "        dist_lst.append(dist_path)\n",
        "        angle_lst.append(angle_path)\n",
        "        ltp_lst.append(ltp_path)\n",
        "        vel_lst.append(vel_path)\n",
        "        fixed_vel_lst.append(fixed_vel_path)\n",
        "        fixed_hof_lst.append(fixed_hof_path)\n",
        "\n",
        "    dataframe_dict = {'video_id': video_lst,\n",
        "                      'action': action_lst,\n",
        "                      'keypoints_path': keypoints_lst,\n",
        "                      'silhouettes_path': silhouettes_lst,\n",
        "                      'video_path': video_path_lst,\n",
        "                      'hof_path': hof_lst,\n",
        "                      'dist_path': dist_lst,\n",
        "                      'angle_path': angle_lst,\n",
        "                      'ltp_path': ltp_lst,\n",
        "                      'vel_path': vel_lst,\n",
        "                      'fixed_vel_path': fixed_vel_lst,\n",
        "                      'fixed_hof_path': fixed_hof_lst}\n",
        "\n",
        "    ground_truth = pd.DataFrame(dataframe_dict)\n",
        "    ground_truth = ground_truth.set_index('video_id')\n",
        "\n",
        "    # K-Fold\n",
        "    skf = KFold(n_splits=num_folds, shuffle=True, random_state=random_state)\n",
        "    ground_truth['fold'] = -1\n",
        "    X = ground_truth.index.values\n",
        "    y = ground_truth['action'].values\n",
        "\n",
        "    for fold_idx, (_, val_idx) in enumerate(skf.split(X, y)):\n",
        "        ground_truth.loc[ground_truth.index[val_idx], 'fold'] = fold_idx\n",
        "\n",
        "    return ground_truth\n",
        "\n",
        "gt = get_ground_truth()\n",
        "print(gt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6YXS9odjNO3"
      },
      "outputs": [],
      "source": [
        "# segmentation\n",
        "\n",
        "def silhouette_extraction(frame, prediction):\n",
        "    new_width = 480\n",
        "    new_height = 320\n",
        "    # frame = cv2.imread(frame_path)\n",
        "    # plt.imshow(frame)\n",
        "    # plt.show()\n",
        "    resized_frame = cv2.resize(frame, (new_width, new_height))\n",
        "    gray_image = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2GRAY)\n",
        "    masks_image = np.zeros_like(gray_image, dtype=np.uint8)\n",
        "\n",
        "    masks = prediction['masks']\n",
        "\n",
        "    # Loop through masks, assuming person class index is 1 (adjust if needed\n",
        "    for i in range(masks.shape[0]):\n",
        "        if prediction['labels'][i] == 1 and prediction['scores'][i] > 0.5:  # Adjust class index for person\n",
        "\n",
        "            mask = masks[i].cpu().numpy().squeeze()\n",
        "\n",
        "            # plt.imshow(frame_copy)\n",
        "            # plt.title('frame copy')\n",
        "            # plt.show()\n",
        "            masks_image[mask > 0.4] = 1\n",
        "\n",
        "    gray_image = gray_image * masks_image\n",
        "\n",
        "    # white_gray_image = gray_image\n",
        "    # white_gray_image[white_gray_image == 0] = 255\n",
        "\n",
        "    # plt.imshow(gray_image, cmap=\"gray\")\n",
        "    # plt.title('gray image')\n",
        "    # plt.show()\n",
        "\n",
        "    return gray_image\n",
        "\n",
        "def process_frames(video_path, silhouette_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    selected_frames = 61\n",
        "    half_frames = total_frames // 2\n",
        "    middle_starting_frame = max(0, min(half_frames - selected_frames // 2, total_frames - selected_frames))\n",
        "    middle_ending_frame = middle_starting_frame + selected_frames\n",
        "    batch_size = 8\n",
        "    silhouette_paths = []\n",
        "    tensor_frames = []\n",
        "    frames = []\n",
        "    j = 0\n",
        "\n",
        "    # Read frames from the video\n",
        "    for frame_no in range(middle_starting_frame, middle_ending_frame):\n",
        "        # Set the frame position\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n",
        "\n",
        "        # Read the current frame from the video\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            raise ValueError(f\"Failed to read frame {frame_no} from the video!\")\n",
        "\n",
        "        # Set frame and silhouette paths\n",
        "        silhouettes_path = os.path.join(silhouette_path, f\"{frame_no}.png\")\n",
        "\n",
        "        # Check if silhouette exists already\n",
        "        if os.path.exists(silhouettes_path):\n",
        "            print(f\"Silhouette {silhouettes_path} already exists\")\n",
        "        else:\n",
        "            silhouette_paths.append(silhouettes_path)\n",
        "            frames.append(frame)  # Adding the frame name to paths (not using full path for video frames)\n",
        "\n",
        "            # Resize the frame (480x320) and convert it to RGB\n",
        "            new_width = 480\n",
        "            new_height = 320\n",
        "            resized_frame = cv2.resize(frame, (new_width, new_height))\n",
        "            resized_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Transform to tensor\n",
        "            transform = torchvision.transforms.ToTensor()\n",
        "            resized_frame = transform(resized_frame).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "            # Append the tensor to the list\n",
        "            tensor_frames.append(resized_frame)\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "    # print(\"silhouette path\", silhouette_paths)\n",
        "    batches = []\n",
        "\n",
        "    for i in range(0, len(tensor_frames), batch_size):\n",
        "        batch = tensor_frames[i:i+batch_size]\n",
        "        batch = torch.cat(batch, dim=0)\n",
        "        # batches.append(batch)\n",
        "        batches.append(batch.to(device))  # Move batch to GPU\n",
        "\n",
        "    for batch_no, batch in enumerate(batches):\n",
        "        with torch.no_grad():\n",
        "          predictions = model(batch)\n",
        "\n",
        "        # Iterate over predictions and corresponding frame information\n",
        "        for i in range(len(batch)):\n",
        "          prediction = predictions[i]  # Access prediction for the current frame\n",
        "\n",
        "          # Extract silhouettes\n",
        "          silhouette = silhouette_extraction(frames[j], prediction)\n",
        "\n",
        "          # Save silhouettes\n",
        "          cv2.imwrite(silhouette_paths[j], silhouette)\n",
        "          print(f\"Processed frame: {silhouette_paths[j]}\")\n",
        "\n",
        "          # plt.imshow(silhouette)\n",
        "          # plt.show()\n",
        "\n",
        "          j = j + 1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load a pre-trained Mask R-CNN model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=detection.MaskRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
        "    model.to(device)  # Move the model to GPU\n",
        "    model.eval()\n",
        "\n",
        "    video_df = get_ground_truth()\n",
        "    # print(video_df)\n",
        "    for video, video_row in video_df.iterrows():\n",
        "\n",
        "      process_frames(video_row['video_path'], video_row['silhouettes_path'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XO6gAVTsAjrs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31b083e2-bdb8-4ef1-d6f5-4c87e5cc01d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video no 1_2\n",
            "(60, 18)\n",
            "video no 2_2\n",
            "(60, 18)\n",
            "video no 3_2\n",
            "(60, 18)\n",
            "video no 4_2\n",
            "(60, 18)\n",
            "video no 16_1\n",
            "(60, 18)\n",
            "video no 15_1\n",
            "(60, 18)\n",
            "video no 14_1\n",
            "(60, 18)\n",
            "video no 13_1\n",
            "(60, 18)\n",
            "video no 12_3\n",
            "(60, 18)\n",
            "video no 11_3\n",
            "(60, 18)\n",
            "video no 9_3\n",
            "(60, 18)\n",
            "video no 7_3\n",
            "(60, 18)\n",
            "video no 17_1\n",
            "(60, 18)\n",
            "video no 18_2\n",
            "(60, 18)\n",
            "video no 19_3\n",
            "(60, 18)\n",
            "video no 20_1\n",
            "(60, 18)\n",
            "video no 23_1\n",
            "(60, 18)\n",
            "video no 26_1\n",
            "(60, 18)\n",
            "video no 29_1\n",
            "(60, 18)\n",
            "video no 32_2\n",
            "(60, 18)\n",
            "video no 33_1\n",
            "(60, 18)\n",
            "video no 34_3\n",
            "(60, 18)\n",
            "video no 35_2\n",
            "(60, 18)\n",
            "video no 36_1\n",
            "(60, 18)\n",
            "video no 37_3\n",
            "(60, 18)\n",
            "video no 38_2\n",
            "(60, 18)\n",
            "video no 40_3\n",
            "(60, 18)\n",
            "video no 39_1\n",
            "(60, 18)\n",
            "video no 41_3\n",
            "(60, 18)\n",
            "video no 43_2\n",
            "(60, 18)\n",
            "video no 42_1\n",
            "(60, 18)\n",
            "video no 44_3\n",
            "(60, 18)\n",
            "video no 45_1\n",
            "(60, 18)\n",
            "video no 46_2\n",
            "(60, 18)\n",
            "video no 22_3\n",
            "(60, 18)\n",
            "video no 21_2\n",
            "(60, 18)\n",
            "video no 24_2\n",
            "(60, 18)\n",
            "video no 25_3\n",
            "(60, 18)\n",
            "video no 27_2\n",
            "(60, 18)\n",
            "video no 28_3\n",
            "(60, 18)\n",
            "video no 30_2\n",
            "(60, 18)\n",
            "video no 31_3\n",
            "(60, 18)\n",
            "video no 47_3\n",
            "(60, 18)\n",
            "video no 48_2\n",
            "(60, 18)\n",
            "video no 49_1\n",
            "(60, 18)\n",
            "video no 50_3\n",
            "(60, 18)\n",
            "video no 51_2\n",
            "(60, 18)\n",
            "video no 52_1\n",
            "(60, 18)\n",
            "video no 53_3\n",
            "(60, 18)\n",
            "video no 54_2\n",
            "(60, 18)\n",
            "video no 55_1\n",
            "(60, 18)\n",
            "video no 57_3\n",
            "(60, 18)\n",
            "video no 56_2\n",
            "(60, 18)\n",
            "video no 58_1\n",
            "(60, 18)\n",
            "video no 59_3\n",
            "(60, 18)\n",
            "video no 60_2\n",
            "(60, 18)\n",
            "video no 61_1\n",
            "(60, 18)\n",
            "video no 62_3\n",
            "(60, 18)\n",
            "video no 63_2\n",
            "(60, 18)\n",
            "video no 64_1\n",
            "(60, 18)\n",
            "video no 65_3\n",
            "(60, 18)\n",
            "video no 66_2\n",
            "(60, 18)\n",
            "video no 67_1\n",
            "(60, 18)\n",
            "video no 68_3\n",
            "(60, 18)\n",
            "video no 69_1\n",
            "(60, 18)\n",
            "video no 71_2\n",
            "(60, 18)\n",
            "video no 72_1\n",
            "(60, 18)\n",
            "video no 73_3\n",
            "(60, 18)\n",
            "video no 74_2\n",
            "(60, 18)\n",
            "video no 76_1\n",
            "(60, 18)\n",
            "video no 75_3\n",
            "(60, 18)\n",
            "video no 77_2\n",
            "(60, 18)\n",
            "video no 77_3\n",
            "(60, 18)\n",
            "video no 78_2\n",
            "(60, 18)\n",
            "video no 79_1\n",
            "(60, 18)\n",
            "video no 80_3\n",
            "(60, 18)\n",
            "video no 81_2\n",
            "(60, 18)\n",
            "video no 82_1\n",
            "(60, 18)\n",
            "video no 83_3\n",
            "(60, 18)\n",
            "video no 85_2\n",
            "(60, 18)\n",
            "video no 84_1\n",
            "(60, 18)\n",
            "video no 86_3\n",
            "(60, 18)\n",
            "video no 87_2\n",
            "(60, 18)\n",
            "video no 88_1\n",
            "(60, 18)\n",
            "video no 89_3\n",
            "(60, 18)\n",
            "video no 90_2\n",
            "(60, 18)\n",
            "video no 91_1\n",
            "(60, 18)\n",
            "video no 92_3\n",
            "(60, 18)\n",
            "video no 93_2\n",
            "(60, 18)\n",
            "video no 94_1\n",
            "(60, 18)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import json\n",
        "\n",
        "def extract_hof_features(prev_frame, curr_frame):\n",
        "    # Calculate the dense optical flow using Farneback method\n",
        "    flow = cv2.calcOpticalFlowFarneback(prev_frame, curr_frame, None,\n",
        "                                        pyr_scale=0.5, levels=3, winsize=15,\n",
        "                                        iterations=3, poly_n=5, poly_sigma=1.2,\n",
        "                                        flags=0)\n",
        "\n",
        "    # Compute magnitude and angle of the flow vectors\n",
        "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1], angleInDegrees=True)\n",
        "\n",
        "    # Bin the angles into 8 bins\n",
        "    num_bins = 18\n",
        "    bin_edges = np.linspace(0, 360, num_bins + 1)\n",
        "    hof_features = np.histogram(angle, bins=bin_edges, weights=magnitude, density=True)[0]\n",
        "\n",
        "     # Avoid NaNs by replacing them with zeros\n",
        "    hof_features[np.isnan(hof_features)] = 0\n",
        "    return hof_features\n",
        "\n",
        "def process_frames_for_hof(video_silhouettes_path, target_hof_features_path):\n",
        "\n",
        "    silhouettes = os.listdir(video_silhouettes_path)\n",
        "    sorted_silhouettes = sorted(silhouettes, key=lambda x: int(x.split('.')[0]))\n",
        "    sorted_silhouettes = sorted_silhouettes[1:]\n",
        "\n",
        "    hof_features_list = []\n",
        "\n",
        "    for i in range(1, len(sorted_silhouettes)):  # Start from the second frame\n",
        "\n",
        "        curr_silhouette_path = os.path.join(video_silhouettes_path, sorted_silhouettes[i])\n",
        "        prev_silhouette_path = os.path.join(video_silhouettes_path, sorted_silhouettes[i-1])\n",
        "\n",
        "        curr_silhouette = cv2.imread(curr_silhouette_path, cv2.IMREAD_GRAYSCALE)\n",
        "        prev_silhouette = cv2.imread(prev_silhouette_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Extract HOF features between current and previous silhouette\n",
        "        hof_features = extract_hof_features(prev_silhouette, curr_silhouette)\n",
        "\n",
        "        hof_features_list.append(hof_features)\n",
        "\n",
        "    hof_features_list = np.array(hof_features_list)\n",
        "\n",
        "    hof_features_list = np.pad(hof_features_list, ((1, 0), (0, 0)), mode='constant', constant_values=0)\n",
        "    print(hof_features_list.shape)\n",
        "\n",
        "    if hof_features_list.shape == (0, ):\n",
        "        raise ValueError(\"No HOF features extracted\")\n",
        "\n",
        "\n",
        "    with open(target_hof_features_path, 'w') as f:\n",
        "        hof_features_list = hof_features_list.tolist()\n",
        "        json.dump({\"hof\": hof_features_list}, f)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    video_df = get_ground_truth()\n",
        "\n",
        "    for video, video_row in video_df.iterrows():\n",
        "\n",
        "        print(\"video no\" , video)\n",
        "\n",
        "        # if os.path.exists(video_row['hof_path']):\n",
        "        #     print(f\"hof features file {video_row['hof_path']} exists already\")\n",
        "        #     continue\n",
        "\n",
        "        process_frames_for_hof(video_row['silhouettes_path'], video_row['fixed_hof_path'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGNqAqXS-mh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "226fdb3b-3047-4a8b-fd04-7e9354b4e40d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing video no: 1_2\n",
            "(60, 512)\n",
            "Processing video no: 2_2\n",
            "(60, 512)\n",
            "Processing video no: 3_2\n",
            "(60, 512)\n",
            "Processing video no: 4_2\n",
            "(60, 512)\n",
            "Processing video no: 16_1\n",
            "(60, 512)\n",
            "Processing video no: 15_1\n",
            "(60, 512)\n",
            "Processing video no: 14_1\n",
            "(60, 512)\n",
            "Processing video no: 13_1\n",
            "(60, 512)\n",
            "Processing video no: 12_3\n",
            "(60, 512)\n",
            "Processing video no: 11_3\n",
            "(60, 512)\n",
            "Processing video no: 9_3\n",
            "(60, 512)\n",
            "Processing video no: 7_3\n",
            "(60, 512)\n",
            "Processing video no: 17_1\n",
            "(60, 512)\n",
            "Processing video no: 18_2\n",
            "(60, 512)\n",
            "Processing video no: 19_3\n",
            "(60, 512)\n",
            "Processing video no: 20_1\n",
            "(60, 512)\n",
            "Processing video no: 23_1\n",
            "(60, 512)\n",
            "Processing video no: 26_1\n",
            "(60, 512)\n",
            "Processing video no: 29_1\n",
            "(60, 512)\n",
            "Processing video no: 32_2\n",
            "(60, 512)\n",
            "Processing video no: 33_1\n",
            "(60, 512)\n",
            "Processing video no: 34_3\n",
            "(60, 512)\n",
            "Processing video no: 35_2\n",
            "(60, 512)\n",
            "Processing video no: 36_1\n",
            "(60, 512)\n",
            "Processing video no: 37_3\n",
            "(60, 512)\n",
            "Processing video no: 38_2\n",
            "(60, 512)\n",
            "Processing video no: 40_3\n",
            "(60, 512)\n",
            "Processing video no: 39_1\n",
            "(60, 512)\n",
            "Processing video no: 41_3\n",
            "(60, 512)\n",
            "Processing video no: 43_2\n",
            "(60, 512)\n",
            "Processing video no: 42_1\n",
            "(60, 512)\n",
            "Processing video no: 44_3\n",
            "(60, 512)\n",
            "Processing video no: 45_1\n",
            "(60, 512)\n",
            "Processing video no: 46_2\n",
            "(60, 512)\n",
            "Processing video no: 22_3\n",
            "(60, 512)\n",
            "Processing video no: 21_2\n",
            "(60, 512)\n",
            "Processing video no: 24_2\n",
            "(60, 512)\n",
            "Processing video no: 25_3\n",
            "(60, 512)\n",
            "Processing video no: 27_2\n",
            "(60, 512)\n",
            "Processing video no: 28_3\n",
            "(60, 512)\n",
            "Processing video no: 30_2\n",
            "(60, 512)\n",
            "Processing video no: 31_3\n",
            "(60, 512)\n",
            "Processing video no: 47_3\n",
            "(60, 512)\n",
            "Processing video no: 48_2\n",
            "(60, 512)\n",
            "Processing video no: 49_1\n",
            "(60, 512)\n",
            "Processing video no: 50_3\n",
            "(60, 512)\n",
            "Processing video no: 51_2\n",
            "(60, 512)\n",
            "Processing video no: 52_1\n",
            "(60, 512)\n",
            "Processing video no: 53_3\n",
            "(60, 512)\n",
            "Processing video no: 54_2\n",
            "(60, 512)\n",
            "Processing video no: 55_1\n",
            "(60, 512)\n",
            "Processing video no: 57_3\n",
            "(60, 512)\n",
            "Processing video no: 56_2\n",
            "(60, 512)\n",
            "Processing video no: 58_1\n",
            "(60, 512)\n",
            "Processing video no: 59_3\n",
            "(60, 512)\n",
            "Processing video no: 60_2\n",
            "(60, 512)\n",
            "Processing video no: 61_1\n",
            "(60, 512)\n",
            "Processing video no: 62_3\n",
            "(60, 512)\n",
            "Processing video no: 63_2\n",
            "(60, 512)\n",
            "Processing video no: 64_1\n",
            "(60, 512)\n",
            "Processing video no: 65_3\n",
            "(60, 512)\n",
            "Processing video no: 66_2\n",
            "(60, 512)\n",
            "Processing video no: 67_1\n",
            "(60, 512)\n",
            "Processing video no: 68_3\n",
            "(60, 512)\n",
            "Processing video no: 69_1\n",
            "(60, 512)\n",
            "Processing video no: 71_2\n",
            "(60, 512)\n",
            "Processing video no: 72_1\n",
            "(60, 512)\n",
            "Processing video no: 73_3\n",
            "(60, 512)\n",
            "Processing video no: 74_2\n",
            "(60, 512)\n",
            "Processing video no: 76_1\n",
            "(60, 512)\n",
            "Processing video no: 75_3\n",
            "(60, 512)\n",
            "Processing video no: 77_2\n",
            "(60, 512)\n",
            "Processing video no: 77_3\n",
            "(60, 512)\n",
            "Processing video no: 78_2\n",
            "(60, 512)\n",
            "Processing video no: 79_1\n",
            "(60, 512)\n",
            "Processing video no: 80_3\n",
            "(60, 512)\n",
            "Processing video no: 81_2\n",
            "(60, 512)\n",
            "Processing video no: 82_1\n",
            "(60, 512)\n",
            "Processing video no: 83_3\n",
            "(60, 512)\n",
            "Processing video no: 85_2\n",
            "(60, 512)\n",
            "Processing video no: 84_1\n",
            "(60, 512)\n",
            "Processing video no: 86_3\n",
            "(60, 512)\n",
            "Processing video no: 87_2\n",
            "(60, 512)\n",
            "Processing video no: 88_1\n",
            "(60, 512)\n",
            "Processing video no: 89_3\n",
            "(60, 512)\n",
            "Processing video no: 90_2\n",
            "(60, 512)\n",
            "Processing video no: 91_1\n",
            "(60, 512)\n",
            "Processing video no: 92_3\n",
            "(60, 512)\n",
            "Processing video no: 93_2\n",
            "(60, 512)\n",
            "Processing video no: 94_1\n",
            "(60, 512)\n"
          ]
        }
      ],
      "source": [
        "# parallel ltp\n",
        "\n",
        "\n",
        "def compute_ltp(image, radius=1, neighbors=8, threshold=5):\n",
        "    \"\"\"\n",
        "    Compute Local Ternary Patterns (LTP) for a grayscale image efficiently.\n",
        "    Uses NumPy vectorized operations and SciPy shift to improve speed.\n",
        "    \"\"\"\n",
        "    image = image.astype(np.float32)\n",
        "    padded_image = cv2.copyMakeBorder(image, radius, radius, radius, radius, cv2.BORDER_REFLECT)\n",
        "    # padded_image = np.pad(image, pad_width=radius, mode='edge')\n",
        "\n",
        "    h, w = image.shape\n",
        "    ltp_pos = np.zeros((h, w), dtype=np.uint8)\n",
        "    ltp_neg = np.zeros((h, w), dtype=np.uint8)\n",
        "\n",
        "    offsets = [\n",
        "        (int(np.round(radius * np.sin(2 * np.pi * n / neighbors))),\n",
        "         int(np.round(radius * np.cos(2 * np.pi * n / neighbors))))\n",
        "        for n in range(neighbors)\n",
        "    ]\n",
        "\n",
        "    # Create stacked neighbor shifts\n",
        "    neighbors_matrix = np.stack([\n",
        "        shift(padded_image, shift=(dy, dx), mode='nearest')[radius:-radius, radius:-radius]\n",
        "        for dy, dx in offsets\n",
        "    ], axis=-1)  # Shape: (h, w, neighbors)\n",
        "\n",
        "    center_matrix = image[..., None]  # Shape: (h, w, 1)\n",
        "    diff = neighbors_matrix - center_matrix  # Compute differences\n",
        "\n",
        "    # Compute LTP codes using NumPy boolean indexing (Vectorized)\n",
        "    ltp_pos = np.sum((diff > threshold) * (1 << np.arange(neighbors)), axis=-1, dtype=np.uint8)\n",
        "    ltp_neg = np.sum((diff < -threshold) * (1 << np.arange(neighbors)), axis=-1, dtype=np.uint8)\n",
        "\n",
        "    return ltp_pos, ltp_neg\n",
        "\n",
        "\n",
        "def extract_ltp_feat_for_frame(image, radius=1, neighbors=8, threshold=5):\n",
        "    \"\"\"\n",
        "    Extract LTP histogram features efficiently.\n",
        "    \"\"\"\n",
        "    ltp_pos, ltp_neg = compute_ltp(image, radius, neighbors, threshold)\n",
        "\n",
        "    # Compute histograms efficiently\n",
        "    pos_hist = np.bincount(ltp_pos.ravel(), minlength=256)\n",
        "    neg_hist = np.bincount(ltp_neg.ravel(), minlength=256)\n",
        "\n",
        "    # Concatenate histograms and convert to float32\n",
        "    return np.concatenate((pos_hist, neg_hist)).astype(np.float32)\n",
        "\n",
        "\n",
        "def ltp_feat_extraction(video_silhouettes_path, target_ltp_features_path):\n",
        "    \"\"\"\n",
        "    Process silhouette frames and extract LTP features in parallel.\n",
        "    \"\"\"\n",
        "\n",
        "    silhouettes = os.listdir(video_silhouettes_path)\n",
        "    silhouettes = sorted(silhouettes, key=lambda x: int(x.split('.')[0]))\n",
        "    silhouettes = silhouettes[1:]\n",
        "    # Use Joblib for parallel processing\n",
        "    ltp_features = Parallel(n_jobs=-1)(\n",
        "        delayed(extract_ltp_feat_for_frame)(\n",
        "            cv2.imread(os.path.join(video_silhouettes_path, silhouette_path), cv2.IMREAD_GRAYSCALE)\n",
        "            ) for silhouette_path in silhouettes\n",
        "    )\n",
        "\n",
        "    ltp_features = np.array(ltp_features)\n",
        "\n",
        "    print(ltp_features.shape)\n",
        "    if ltp_features.shape == (0, ):\n",
        "        raise ValueError(\"No LTP features extracted\")\n",
        "\n",
        "    with open(target_ltp_features_path, 'w') as f:\n",
        "        json.dump({\"ltp\": ltp_features.tolist()}, f)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    video_df = get_ground_truth()  # Ensure this function provides video data paths and output locations\n",
        "\n",
        "    for video, video_row in video_df.iterrows():\n",
        "        print(\"Processing video no:\", video)\n",
        "\n",
        "        # if os.path.exists(video_row['ltp_path']):\n",
        "        #     print(f\"LTP features file {video_row['ltp_path']} already exists\")\n",
        "        #     continue\n",
        "\n",
        "        ltp_feat_extraction(video_row['silhouettes_path'], video_row['ltp_path'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7XJHU_tEIuf"
      },
      "outputs": [],
      "source": [
        "# intra and inter distance changes\n",
        "\n",
        "# Indices and mapping as provided\n",
        "indices = {\n",
        "    'N1': 0, 'LS1': 1, 'RS1': 2, 'LE1': 3, 'RE1': 4,\n",
        "    'LW1': 5, 'RW1': 6,\n",
        "    'N2': 0, 'LS2': 1, 'RS2': 2, 'LE2': 3, 'RE2': 4,\n",
        "    'LW2': 5, 'RW2': 6,\n",
        "}\n",
        "\n",
        "intra_distance_mapping = {\n",
        "    \"N1\": [\"RW1\"],\n",
        "    \"LS1\": [\"LW1\"],\n",
        "    \"RE1\": [\"RW1\"],\n",
        "    \"LE1\": [\"LW1\"],\n",
        "\n",
        "     \"N2\": [\"RW2\"],\n",
        "    \"LS2\": [\"LW2\"],\n",
        "    \"RE2\": [\"RW2\"],\n",
        "    \"LE2\": [\"LW2\"],\n",
        "}\n",
        "\n",
        "inter_distance_mapping = {\n",
        "\n",
        "    \"LS1\": [\"RS2\"],\n",
        "    \"LW1\": [\"RW2\"],\n",
        "    \"LE1\": [\"RE2\"],\n",
        "    \"N1\": [\"N2\"],\n",
        "}\n",
        "\n",
        "# Function to compute distances\n",
        "def compute_distance_changes(input_file, output_file):\n",
        "    # Load keypoints from JSON\n",
        "    with open(input_file, 'r') as f:\n",
        "        keypoints = json.load(f)\n",
        "\n",
        "    sorted_frames = sorted(keypoints.keys(), key=lambda x: int(x))\n",
        "\n",
        "    total_frames = len(sorted_frames)\n",
        "\n",
        "    # Calculate the start and end indices for the middle 60 frames\n",
        "    start_index = max((total_frames - 60) // 2, 0)  # Ensure index is non-negative\n",
        "    end_index = start_index + 60\n",
        "\n",
        "    # Select the middle 60 frames\n",
        "    sorted_frames = sorted_frames[start_index:end_index]\n",
        "\n",
        "    intra_inter_distances = []\n",
        "\n",
        "    for frame in sorted_frames:\n",
        "        coords = keypoints[frame]\n",
        "        # Identify left and right persons based on x-coordinates of their noses\n",
        "        left_person_id, right_person_id = sorted(\n",
        "            coords.keys(),\n",
        "            key=lambda pid: coords[pid][0][0]\n",
        "        )\n",
        "\n",
        "        frame_intra_distances = []\n",
        "        frame_inter_distances = []\n",
        "\n",
        "        # Iterate through intra-person distances\n",
        "        for joint_name, target_joints in intra_distance_mapping.items():\n",
        "            if joint_name[-1] == \"1\":\n",
        "                person_id = left_person_id\n",
        "            else:\n",
        "                person_id = right_person_id\n",
        "\n",
        "            joint_idx = indices[joint_name]\n",
        "            for target_joint in target_joints:\n",
        "                target_joint_idx = indices[target_joint]\n",
        "                try:\n",
        "                    euclidean_distance = np.linalg.norm(\n",
        "                        np.array(coords[person_id][joint_idx]) -\n",
        "                        np.array(coords[person_id][target_joint_idx])\n",
        "                    )\n",
        "                    frame_intra_distances.append(euclidean_distance)\n",
        "                except IndexError:\n",
        "                    print(\"Error\")\n",
        "                    continue\n",
        "\n",
        "        # Iterate through inter-person distances\n",
        "        for joint_name, target_joints in inter_distance_mapping.items():\n",
        "            if joint_name[-1] == \"1\":\n",
        "                joint_person_id = left_person_id\n",
        "            else:\n",
        "                joint_person_id = right_person_id\n",
        "\n",
        "            joint_idx = indices[joint_name]\n",
        "            for target_joint in target_joints:\n",
        "                if target_joint[-1] == \"1\":\n",
        "                    target_person_id = left_person_id\n",
        "                else:\n",
        "                    target_person_id = right_person_id\n",
        "\n",
        "                target_joint_idx = indices[target_joint]\n",
        "                try:\n",
        "                    euclidean_distance = np.linalg.norm(\n",
        "                        np.array(coords[joint_person_id][joint_idx]) -\n",
        "                        np.array(coords[target_person_id][target_joint_idx])\n",
        "                    )\n",
        "                    frame_inter_distances.append(euclidean_distance)\n",
        "                except IndexError:\n",
        "                    print(\"Error\")\n",
        "                    print(joint_person_id)\n",
        "                    print(target_person_id)\n",
        "\n",
        "        combined_distances = frame_intra_distances + frame_inter_distances\n",
        "        intra_inter_distances.append(combined_distances)\n",
        "\n",
        "    intra_inter_distances = np.array(intra_inter_distances)\n",
        "\n",
        "    print(\"shape\", intra_inter_distances.shape)\n",
        "    # Save results to output JSON\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump({\"intra_inter_distance_mapping\": intra_inter_distances.tolist()}, f)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    video_df = get_ground_truth()\n",
        "\n",
        "    for video, video_row in video_df.iterrows():\n",
        "\n",
        "        print(\"video no\" , video)\n",
        "\n",
        "        # if os.path.exists(video_row['dist']):\n",
        "        #     print(f\"distance change file {video_row['dist']} exists already\")\n",
        "        #     continue\n",
        "\n",
        "        compute_distance_changes(video_row['keypoints'], video_row['dist'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTjcv67mKzSV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Indices and mapping as provided\n",
        "indices = {\n",
        "    'N1': 0, 'LS1': 1, 'RS1': 2, 'LE1': 3, 'RE1': 4,\n",
        "    'LW1': 5, 'RW1': 6,\n",
        "    'N2': 0, 'LS2': 1, 'RS2': 2, 'LE2': 3, 'RE2': 4,\n",
        "    'LW2': 5, 'RW2': 6,\n",
        "}\n",
        "\n",
        "intra_angle_mapping = {\n",
        "    \"N1\": [\"RS1\", \"LS1\"],\n",
        "    \"RS1\": [\"RE1\"],\n",
        "    \"LS1\": [\"LE1\"],\n",
        "    \"RE1\": [\"RW1\"],\n",
        "    \"LE1\": [\"LW1\"],\n",
        "\n",
        "    \"N2\": [\"RS2\", \"LS2\"],\n",
        "    \"RS2\": [\"RE2\"],\n",
        "    \"LS2\": [\"LE2\"],\n",
        "    \"RE2\": [\"RW2\"],\n",
        "    \"LE2\": [\"LW2\"]\n",
        "\n",
        "}\n",
        "\n",
        "inter_angle_mapping = {\n",
        "    \"LW1\": [\"RW2\"],\n",
        "    \"LE1\": [\"RE2\"],\n",
        "    \"LS1\": [\"RS1\"]\n",
        "}\n",
        "\n",
        "\n",
        "# Function to compute angles\n",
        "def compute_angle_changes(input_file, output_file):\n",
        "    # Load keypoints from JSON\n",
        "    with open(input_file, 'r') as f:\n",
        "        keypoints = json.load(f)\n",
        "\n",
        "    sorted_frames = sorted(keypoints.keys(), key=lambda x: int(x))\n",
        "\n",
        "    total_frames = len(sorted_frames)\n",
        "\n",
        "    # Calculate the start and end indices for the middle 60 frames\n",
        "    start_index = max((total_frames - 60) // 2, 0)  # Ensure index is non-negative\n",
        "    end_index = start_index + 60\n",
        "\n",
        "    # Select the middle 60 frames\n",
        "    sorted_frames = sorted_frames[start_index:end_index]\n",
        "\n",
        "    intra_inter_angles = []\n",
        "\n",
        "    for frame in sorted_frames:\n",
        "        coords = keypoints[frame]\n",
        "        # Identify left and right persons based on x-coordinates of their noses\n",
        "        left_person_id, right_person_id = sorted(\n",
        "            coords.keys(),\n",
        "            key=lambda pid: coords[pid][0][0]\n",
        "        )\n",
        "\n",
        "        frame_intra_angles = []\n",
        "        frame_inter_angles = []\n",
        "\n",
        "        # Iterate through intra-person angles\n",
        "        for joint_name, target_joints in intra_angle_mapping.items():\n",
        "            if joint_name[-1] == \"1\":\n",
        "                person_id = left_person_id\n",
        "            else:\n",
        "                person_id = right_person_id\n",
        "\n",
        "            joint_idx = indices[joint_name]\n",
        "            for target_joint in target_joints:\n",
        "                target_joint_idx = indices[target_joint]\n",
        "                try:\n",
        "                    delta_x = coords[person_id][target_joint_idx][0] - coords[person_id][joint_idx][0]\n",
        "                    delta_y = coords[person_id][target_joint_idx][1] - coords[person_id][joint_idx][1]\n",
        "                    angle = np.arctan2(delta_y, delta_x)\n",
        "                    frame_intra_angles.append(angle)\n",
        "                except IndexError:\n",
        "                    print(f\"Error computing intra angle for joint {joint_name}\")\n",
        "                    continue\n",
        "\n",
        "        # Iterate through inter-person angles\n",
        "        for joint_name, target_joints in inter_angle_mapping.items():\n",
        "            if joint_name[-1] == \"1\":\n",
        "                joint_person_id = left_person_id\n",
        "            else:\n",
        "                joint_person_id = right_person_id\n",
        "\n",
        "            joint_idx = indices[joint_name]\n",
        "            for target_joint in target_joints:\n",
        "                if target_joint[-1] == \"1\":\n",
        "                    target_person_id = left_person_id\n",
        "                else:\n",
        "                    target_person_id = right_person_id\n",
        "\n",
        "                target_joint_idx = indices[target_joint]\n",
        "                try:\n",
        "                    delta_x = coords[target_person_id][target_joint_idx][0] - coords[joint_person_id][joint_idx][0]\n",
        "                    delta_y = coords[target_person_id][target_joint_idx][1] - coords[joint_person_id][joint_idx][1]\n",
        "                    angle = np.arctan2(delta_y, delta_x)\n",
        "                    frame_inter_angles.append(angle)\n",
        "                except IndexError:\n",
        "                    print(f\"Error computing inter angle between {joint_name} and {target_joint}\")\n",
        "                    continue\n",
        "\n",
        "        combined_angles = frame_intra_angles + frame_inter_angles\n",
        "        intra_inter_angles.append(combined_angles)\n",
        "\n",
        "    intra_inter_angles = np.array(intra_inter_angles)\n",
        "\n",
        "    print(\"shape\", intra_inter_angles.shape)\n",
        "    # Save results to output JSON\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump({\"intra_inter_angle_mapping\": intra_inter_angles.tolist()}, f)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    video_df = get_ground_truth()\n",
        "\n",
        "    for video, video_row in video_df.iterrows():\n",
        "        print(\"video no\", video)\n",
        "\n",
        "        if os.path.exists(video_row['angle']):\n",
        "            print(f\"angle change file {video_row['angles']} exists already\")\n",
        "            continue\n",
        "\n",
        "        compute_angle_changes(video_row['keypoints'], video_row['angle'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Keypoint indices (for reference)\n",
        "# 0: Nose, 1: Left Shoulder, 2: Right Shoulder, 3: Left Elbow, 4: Right Elbow,\n",
        "# 5: Left Wrist, 6: Right Wrist, 7: Left Hip, 8: Right Hip, 9: Left Knee,\n",
        "# 10: Right Knee, 11: Left Ankle, 12: Right Ankle\n",
        "\n",
        "def compute_velocity_features(input_file, output_file):\n",
        "    # Load keypoints from JSON\n",
        "    with open(input_file, 'r') as f:\n",
        "        keypoints_data = json.load(f)  # Load the nested structure\n",
        "\n",
        "    # Convert the nested structure into a NumPy array\n",
        "    frames = sorted(keypoints_data.keys(), key=lambda x: int(x))  # Sort frames\n",
        "    persons = sorted(keypoints_data[frames[0]].keys(), key=lambda x: int(x))  # Sort persons\n",
        "    num_frames = len(frames)\n",
        "    num_persons = len(persons)\n",
        "    num_keypoints = len(keypoints_data[frames[0]][persons[0]])  # Number of keypoints per person\n",
        "\n",
        "    # Initialize a NumPy array to store keypoints\n",
        "    keypoints = np.zeros((num_frames, num_persons, num_keypoints, 2))  # (frames, persons, keypoints, coordinates)\n",
        "\n",
        "    # Populate the keypoints array\n",
        "    for frame_idx, frame in enumerate(frames):\n",
        "        for person_idx, person in enumerate(persons):\n",
        "            keypoints[frame_idx, person_idx] = keypoints_data[frame][person]\n",
        "\n",
        "    # Reorder persons based on the sum of x-coordinates in the first frame\n",
        "    sum_x_coords = np.sum(keypoints[0, :, :, 0], axis=1)  # Sum of x-coordinates for each person in the first frame\n",
        "    reorder_indices = np.argsort(sum_x_coords)  # Indices to reorder persons\n",
        "\n",
        "    # print(reorder_indices)\n",
        "    # Reorder keypoints array\n",
        "    keypoints = keypoints[:, reorder_indices, :, :]\n",
        "\n",
        "    # Extract middle 60 frames\n",
        "    required_frames = 60\n",
        "\n",
        "    if num_frames < required_frames:\n",
        "        raise ValueError(\"Total frames are less than 60. Cannot extract middle 60 frames.\")\n",
        "        return\n",
        "\n",
        "    middle_start = (num_frames - required_frames) // 2  # Start index of middle 60 frames\n",
        "    middle_end = middle_start + required_frames  # End index of middle 60 frames\n",
        "    keypoints = keypoints[middle_start:middle_end, :, :, :]  # Shape: (60, persons, keypoints, coordinates)\n",
        "\n",
        "    # Reshape keypoints array to (num_frames, num_persons * num_keypoints, 2)\n",
        "    keypoints_reshaped = keypoints.reshape(required_frames, num_persons * num_keypoints, 2)\n",
        "\n",
        "    # Compute velocity features\n",
        "    if keypoints_reshaped.shape[0] < 2:\n",
        "        raise ValueError(\"Not enough frames to compute velocity.\")\n",
        "        return\n",
        "\n",
        "    # Extract X and Y coordinates for all keypoints\n",
        "    prev_coords = keypoints_reshaped[:-1, :, :]  # All frames except last\n",
        "    curr_coords = keypoints_reshaped[1:, :, :]   # All frames except first\n",
        "\n",
        "    # Compute Euclidean distance for velocity (frame-to-frame displacement)\n",
        "    velocity = np.linalg.norm(curr_coords - prev_coords, axis=2)  # Shape: (59, num_persons * num_keypoints)\n",
        "\n",
        "    # velocity_padded = velocity\n",
        "    # Pad velocity array to match the original number of frames (60)\n",
        "    velocity_padded = np.pad(velocity, ((1, 0), (0, 0)), mode='constant', constant_values=0)\n",
        "\n",
        "    print(\"Velocity feature shape:\", velocity_padded.shape)\n",
        "\n",
        "    # Save results to JSON\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump({\"velocity\": velocity_padded.tolist()}, f)\n",
        "\n",
        "    print(\"Velocity features computed and saved successfully.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "     video_df = get_ground_truth()\n",
        "\n",
        "     for video, video_row in video_df.iterrows():\n",
        "        print(\"Processing video:\", video)\n",
        "\n",
        "        if os.path.exists(video_row['fixed_velocity_path']):\n",
        "            print(f\"Velocity features file {video_row['velocity_path']} already exists.\")\n",
        "            continue\n",
        "\n",
        "        compute_velocity_features(video_row['keypoints_path'], video_row['fixed_velocity_path'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr-KXKoZqkld",
        "outputId": "15568f4c-20cc-4f13-f6be-8d2ce3645e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing video: 1_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/1_2.json already exists.\n",
            "Processing video: 2_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/2_2.json already exists.\n",
            "Processing video: 3_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/3_2.json already exists.\n",
            "Processing video: 4_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/4_2.json already exists.\n",
            "Processing video: 16_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/16_1.json already exists.\n",
            "Processing video: 15_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/15_1.json already exists.\n",
            "Processing video: 14_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/14_1.json already exists.\n",
            "Processing video: 13_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/13_1.json already exists.\n",
            "Processing video: 12_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/12_3.json already exists.\n",
            "Processing video: 11_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/11_3.json already exists.\n",
            "Processing video: 9_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/9_3.json already exists.\n",
            "Processing video: 7_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/7_3.json already exists.\n",
            "Processing video: 17_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/17_1.json already exists.\n",
            "Processing video: 18_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/18_2.json already exists.\n",
            "Processing video: 19_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/19_3.json already exists.\n",
            "Processing video: 20_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/20_1.json already exists.\n",
            "Processing video: 23_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/23_1.json already exists.\n",
            "Processing video: 26_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/26_1.json already exists.\n",
            "Processing video: 29_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/29_1.json already exists.\n",
            "Processing video: 32_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/32_2.json already exists.\n",
            "Processing video: 33_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/33_1.json already exists.\n",
            "Processing video: 34_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/34_3.json already exists.\n",
            "Processing video: 35_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/35_2.json already exists.\n",
            "Processing video: 36_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/36_1.json already exists.\n",
            "Processing video: 37_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/37_3.json already exists.\n",
            "Processing video: 38_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/38_2.json already exists.\n",
            "Processing video: 40_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/40_3.json already exists.\n",
            "Processing video: 39_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/39_1.json already exists.\n",
            "Processing video: 41_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/41_3.json already exists.\n",
            "Processing video: 43_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/43_2.json already exists.\n",
            "Processing video: 42_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/42_1.json already exists.\n",
            "Processing video: 44_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/44_3.json already exists.\n",
            "Processing video: 45_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/45_1.json already exists.\n",
            "Processing video: 46_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/46_2.json already exists.\n",
            "Processing video: 22_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/22_3.json already exists.\n",
            "Processing video: 21_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/21_2.json already exists.\n",
            "Processing video: 24_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/24_2.json already exists.\n",
            "Processing video: 25_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/25_3.json already exists.\n",
            "Processing video: 27_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/27_2.json already exists.\n",
            "Processing video: 28_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/28_3.json already exists.\n",
            "Processing video: 30_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/30_2.json already exists.\n",
            "Processing video: 31_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/31_3.json already exists.\n",
            "Processing video: 47_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/47_3.json already exists.\n",
            "Processing video: 48_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/48_2.json already exists.\n",
            "Processing video: 49_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/49_1.json already exists.\n",
            "Processing video: 50_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/50_3.json already exists.\n",
            "Processing video: 51_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/51_2.json already exists.\n",
            "Processing video: 52_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/52_1.json already exists.\n",
            "Processing video: 53_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/53_3.json already exists.\n",
            "Processing video: 54_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/54_2.json already exists.\n",
            "Processing video: 55_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/55_1.json already exists.\n",
            "Processing video: 57_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/57_3.json already exists.\n",
            "Processing video: 56_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/56_2.json already exists.\n",
            "Processing video: 58_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/58_1.json already exists.\n",
            "Processing video: 59_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/59_3.json already exists.\n",
            "Processing video: 60_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/60_2.json already exists.\n",
            "Processing video: 61_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/61_1.json already exists.\n",
            "Processing video: 62_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/62_3.json already exists.\n",
            "Processing video: 63_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/63_2.json already exists.\n",
            "Processing video: 64_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/64_1.json already exists.\n",
            "Processing video: 65_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/65_3.json already exists.\n",
            "Processing video: 66_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/66_2.json already exists.\n",
            "Processing video: 67_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/67_1.json already exists.\n",
            "Processing video: 68_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/68_3.json already exists.\n",
            "Processing video: 69_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/69_1.json already exists.\n",
            "Processing video: 71_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/71_2.json already exists.\n",
            "Processing video: 72_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/72_1.json already exists.\n",
            "Processing video: 73_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/73_3.json already exists.\n",
            "Processing video: 74_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/74_2.json already exists.\n",
            "Processing video: 76_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/76_1.json already exists.\n",
            "Processing video: 75_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/75_3.json already exists.\n",
            "Processing video: 77_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/77_2.json already exists.\n",
            "Processing video: 77_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/77_3.json already exists.\n",
            "Processing video: 78_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/78_2.json already exists.\n",
            "Processing video: 79_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/79_1.json already exists.\n",
            "Processing video: 80_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/80_3.json already exists.\n",
            "Processing video: 81_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/81_2.json already exists.\n",
            "Processing video: 82_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/82_1.json already exists.\n",
            "Processing video: 83_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/83_3.json already exists.\n",
            "Processing video: 85_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/85_2.json already exists.\n",
            "Processing video: 84_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/84_1.json already exists.\n",
            "Processing video: 86_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/86_3.json already exists.\n",
            "Processing video: 87_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/87_2.json already exists.\n",
            "Processing video: 88_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/88_1.json already exists.\n",
            "Processing video: 89_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/89_3.json already exists.\n",
            "Processing video: 90_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/90_2.json already exists.\n",
            "Processing video: 91_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/91_1.json already exists.\n",
            "Processing video: 92_3\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/92_3.json already exists.\n",
            "Processing video: 93_2\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/93_2.json already exists.\n",
            "Processing video: 94_1\n",
            "Velocity features file /content/drive/MyDrive/FYP/Velocity/94_1.json already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVL-zFc7uHob"
      },
      "outputs": [],
      "source": [
        "# Machine learning pipeline classifier code\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, GroupKFold, GridSearchCV, LeaveOneGroupOut\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.svm import SVC\n",
        "# from cuml.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, make_scorer\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib, json\n",
        "\n",
        "\n",
        "\n",
        "def plot_class_separation_3d(X, Y, title):\n",
        "    class_names = ['doing own work', 'passing paper', \"looking at other's work\"]\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "    # Create a color map\n",
        "    cmap = plt.get_cmap('viridis', num_classes)\n",
        "\n",
        "    # Create a figure and a 3D axis\n",
        "    fig = plt.figure(figsize=(9, 6))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Plot each class with its own color\n",
        "    for i in range(num_classes):\n",
        "        indices = Y == i\n",
        "        ax.scatter(X[indices, 0], X[indices, 1],\n",
        "                   color=cmap(i), label=class_names[i], alpha=0.7)\n",
        "\n",
        "    # Create a legend with class names\n",
        "    ax.legend(title='Class')\n",
        "    # ax.set_aspect('auto')\n",
        "    # Set titles and labels\n",
        "    # ax.set_title(title)\n",
        "    ax.set_xlabel('LD1')\n",
        "    ax.set_ylabel('LD2')\n",
        "    ax.set_zlabel('LD3')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def gridSearch(X, Y, group_ids):\n",
        "\n",
        "    num_videos, num_frames, num_features = X.shape\n",
        "\n",
        "    # Step 4: Divide into smaller video samples using a sliding window approach\n",
        "    all_video_samples = []\n",
        "    window_size = 30\n",
        "    step_size = 2\n",
        "    all_y_pred = []\n",
        "    all_y_true = []\n",
        "\n",
        "\n",
        "    for video_idx in range(num_videos):\n",
        "        video_sample = X[video_idx]\n",
        "        video_samples = []\n",
        "\n",
        "        for start in range(0, num_frames - window_size + 1, step_size):\n",
        "            end = start + window_size\n",
        "            sample = video_sample[start:end]\n",
        "            video_samples.append(sample)\n",
        "        all_video_samples.append(video_samples)\n",
        "\n",
        "    all_video_samples = np.array(all_video_samples)\n",
        "    num_samples_per_video = all_video_samples.shape[1]\n",
        "    num_total_samples = num_videos * num_samples_per_video\n",
        "\n",
        "    all_video_samples = all_video_samples.reshape(num_total_samples, window_size * num_features)\n",
        "\n",
        "\n",
        "    print(\"all videos sample shape\", all_video_samples.shape)\n",
        "    Y = np.repeat(Y, num_samples_per_video)\n",
        "    group_ids = np.repeat(group_ids, num_samples_per_video)\n",
        "\n",
        "    # Define the parameter grid for SVM\n",
        "    param_grid = {\n",
        "    'svm__C': [0.01, 0.1, 1, 10, 100],                # Include default value 1.0\n",
        "    'svm__gamma': [1e-3, 1e-4, 1e-5, 1e-1, 1e-2, 'scale', 'auto'], # Include default values 'scale' and 'auto'\n",
        "    'svm__kernel': ['linear', 'poly', 'rbf', 'sigmoid'] # Try different kernels\n",
        "    }\n",
        "\n",
        "\n",
        "    # Create a pipeline that includes scaling, LDA, and SVM\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),        # Feature scaling\n",
        "        ('lda', LDA(n_components=2)),        # Linear Discriminant Analysis\n",
        "        ('svm', SVC(kernel=\"rbf\"))           # SVM with RBF kernel\n",
        "    ])\n",
        "\n",
        "    # Use LeaveOneGroupOut cross-validation\n",
        "    logo = LeaveOneGroupOut()\n",
        "\n",
        "    # Define scoring metrics\n",
        "    scoring = {\n",
        "        'accuracy': 'accuracy',\n",
        "        'precision_macro': make_scorer(precision_score, average='macro', zero_division=0),\n",
        "        'recall_macro': make_scorer(recall_score, average='macro', zero_division=0),\n",
        "        'f1_macro': make_scorer(f1_score, average='macro', zero_division=0)\n",
        "    }\n",
        "\n",
        "    # Set up GridSearchCV with the pipeline, parameter grid, and cross-validation scheme\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=logo, scoring=scoring, refit=False)\n",
        "\n",
        "    # Fit the grid search to the data\n",
        "    grid_search.fit(all_video_samples, Y, groups=group_ids)\n",
        "\n",
        "    best_index = grid_search.cv_results_['rank_test_accuracy'].argmin()\n",
        "\n",
        "    best_params = grid_search.cv_results_['params'][best_index]\n",
        "\n",
        "    best_accuracy = grid_search.cv_results_['mean_test_accuracy'][best_index]\n",
        "    # Get the best parameters\n",
        "    print(f'Best parameters: {best_params}')\n",
        "\n",
        "    # Get the best metrics\n",
        "    print(f'Best accuracy: {best_accuracy:.4f}')\n",
        "\n",
        "    return best_params\n",
        "\n",
        "def classificationWithBestParam(X, Y, group_ids, best_params):\n",
        "\n",
        "    num_videos, num_frames, num_features = X.shape\n",
        "\n",
        "    # Step 4: Divide into smaller video samples using a sliding window approach\n",
        "    all_video_samples = []\n",
        "    all_energy_features_video_samples = []\n",
        "    window_size = 30\n",
        "    step_size = 5\n",
        "    all_y_pred = []\n",
        "    all_y_true = []\n",
        "\n",
        "\n",
        "    for video_idx in range(num_videos):\n",
        "        video_sample = X[video_idx]\n",
        "        video_samples = []\n",
        "\n",
        "        for start in range(0, num_frames - window_size + 1, step_size):\n",
        "            end = start + window_size\n",
        "            sample = video_sample[start:end]\n",
        "            video_samples.append(sample)\n",
        "        all_video_samples.append(video_samples)\n",
        "\n",
        "    all_video_samples = np.array(all_video_samples)\n",
        "    num_samples_per_video = all_video_samples.shape[1]\n",
        "    num_total_samples = num_videos * num_samples_per_video\n",
        "\n",
        "    print(\"all videos samples before\", all_video_samples.shape)\n",
        "\n",
        "    all_video_samples = all_video_samples.reshape(num_total_samples, window_size * num_features)\n",
        "\n",
        "\n",
        "    print(\"all videos sample shape\", all_video_samples.shape)\n",
        "    Y = np.repeat(Y, num_samples_per_video)\n",
        "    group_ids = np.repeat(group_ids, num_samples_per_video)\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    logo = LeaveOneGroupOut()\n",
        "    lda = LDA(n_components=2)\n",
        "\n",
        "    accuracies = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "\n",
        "    save_dir = '/content/drive/MyDrive/FYP/ML_training'\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    fold = 0\n",
        "\n",
        "    for train_index, test_index in logo.split(all_video_samples, Y, groups=group_ids):\n",
        "        X_train, X_test = all_video_samples[train_index], all_video_samples[test_index]\n",
        "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "        print(\"X_train shape\", X_train.shape)\n",
        "        print(\"X_test shape\", X_test.shape)\n",
        "\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "\n",
        "        X_train_lda = lda.fit_transform(X_train, Y_train)\n",
        "        X_test_lda = lda.transform(X_test)\n",
        "\n",
        "        best_svm = SVC(kernel = best_params['svm__kernel'], C=best_params['svm__C'], gamma= best_params['svm__gamma'])\n",
        "        best_svm.fit(X_train_lda, Y_train)\n",
        "\n",
        "        plot_class_separation_3d(X_train_lda, Y_train, 'Training Data')\n",
        "        plot_class_separation_3d(X_test_lda, Y_test, 'Testing Data')\n",
        "\n",
        "        Y_pred = best_svm.predict(X_test_lda)\n",
        "        accuracy = accuracy_score(Y_test, Y_pred)\n",
        "        precision = precision_score(Y_test, Y_pred, average='macro',zero_division=0)\n",
        "        recall = recall_score(Y_test, Y_pred, average='macro')\n",
        "        f1 = f1_score(Y_test, Y_pred, average='macro')\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        all_y_pred.extend(Y_pred)\n",
        "        all_y_true.extend(Y_test)\n",
        "        print(f'Fold accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "         # Save trained models using joblib\n",
        "        lda_model_path = os.path.join(save_dir, f\"lda_fold_{fold}.joblib\")\n",
        "        svm_model_path = os.path.join(save_dir, f\"svm_fold_{fold}.joblib\")\n",
        "        scalar_model_path = os.path.join(save_dir, f\"scaler_fold_{fold}.joblib\")\n",
        "\n",
        "        joblib.dump(lda, lda_model_path)\n",
        "        joblib.dump(best_svm, svm_model_path)\n",
        "        joblib.dump(scaler, scalar_model_path)\n",
        "\n",
        "        fold += 1\n",
        "\n",
        "\n",
        "    # Compute and display confusion matrix\n",
        "    cm = confusion_matrix(all_y_true, all_y_pred, labels=np.unique(Y))\n",
        "\n",
        "    # Normalize the confusion matrix to show percentages\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    class_names = ['doing own work', 'passing paper', \"looking at other's work\"]\n",
        "    # Plot confusion matrix with accuracy percentages\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=class_names)\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "    disp.plot(ax=ax, cmap=plt.cm.Blues, values_format=\".2f\")\n",
        "    # Set aspect ratio to be equal\n",
        "    ax.set_aspect('auto')\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f'Mean precision: {np.mean(precisions):.4f}')\n",
        "    print(f'Mean recall: {np.mean(recalls):.4f}')\n",
        "    print(f'Mean SVM accuracy: {np.mean(accuracies) * 100:.2f}%')\n",
        "    print(f\"fl score: {np.mean(f1_scores)}\")\n",
        "\n",
        "\n",
        "# Read extracted features for each video\n",
        "def reading_features(hof_path, dist_path, angle_path):\n",
        "\n",
        "    with open(dist_path, 'r') as f:\n",
        "        distance_features_data = json.load(f)\n",
        "\n",
        "    with open(hof_path, 'r') as f:\n",
        "        hof_features_data = json.load(f)\n",
        "\n",
        "    with open(angle_path, 'r') as f:\n",
        "        angle_features_data = json.load(f)\n",
        "\n",
        "    distance_features = np.array(distance_features_data['intra_inter_distance_mapping'])\n",
        "    # distance_features = np.array(distance_features_data['velocity'])\n",
        "    angle_features = np.array(angle_features_data['intra_inter_angle_mapping'])\n",
        "    hof_features = np.array(hof_features_data['hof'])\n",
        "\n",
        "    # intra_inter_angle_mapping\n",
        "    combined_features = np.concatenate((\n",
        "                                        distance_features,\n",
        "                                        angle_features,\n",
        "                                        hof_features,\n",
        "                                        ), axis=1)\n",
        "\n",
        "    return combined_features\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming you have a method to get your data as a DataFrame\n",
        "    video_df = get_ground_truth()\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    group_ids = []\n",
        "\n",
        "    for video, video_row in video_df.iterrows():\n",
        "        combined_features  = reading_features(video_row['hof_path'], video_row['dist_path'], video_row['angle_path'])\n",
        "        X.append(combined_features)\n",
        "        Y.append(video_row['action'])\n",
        "        group_ids.append(video_row['fold'])\n",
        "\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    # best_param = gridSearch(X, Y, group_ids)\n",
        "    best_param = {'svm__C': 0.1, 'svm__gamma': 0.1, 'svm__kernel': 'sigmoid'}\n",
        "    classificationWithBestParam(X, Y, group_ids, best_param)\n",
        "\n",
        "\n",
        "# Mean precision: 0.7607\n",
        "# Mean recall: 0.7187\n",
        "# Mean SVM accuracy: 71.43%\n",
        "# fl score: 0.7148465509648211\n",
        "\n",
        "# Mean precision: 0.8413\n",
        "# Mean recall: 0.8267\n",
        "# Mean SVM accuracy: 81.59%\n",
        "# fl score: 0.8187946736089304\n",
        "\n",
        "# Best parameters: {'svm__C': 0.1, 'svm__gamma': 0.1, 'svm__kernel': 'sigmoid'}\n",
        "\n",
        "\n",
        "# 420, 210  (30, 5) 76.51\n",
        "# 420, 210 (40, 3) 78.89\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# {'hidden_dim': 256, 'num_layers': 1, 'lr': 0.001, 'bidirectional': True}\n",
        "# Best Accuracy: 91.82%  (50, 1)\n",
        "\n",
        "# {'hidden_dim': 128, 'num_layers': 1, 'lr': 0.001, 'bidirectional': True}\n",
        "# Best Accuracy: 90.71% (40, 2)\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, bidirectional=False):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # LSTM Layer (Bi-directional if specified)\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "\n",
        "        # Output Layer\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "\n",
        "        # ReLU activation and Dropout for regularization\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # LSTM Layer - Get output and hidden state (we don't need the hidden state here)\n",
        "        lstm_out, _ = self.lstm(x)  # LSTM output shape: (batch_size, seq_len, hidden_dim * 2)\n",
        "\n",
        "        # If bidirectional, we take the output of the last timestep for each direction\n",
        "        if self.bidirectional:\n",
        "            # We take the concatenation of the forward and backward hidden states at the last time step\n",
        "            final_hidden_state = torch.cat((lstm_out[:, -1, :self.hidden_dim], lstm_out[:, 0, self.hidden_dim:]), dim=-1)\n",
        "\n",
        "            # final_hidden_state = lstm_out[:, -1, :self.hidden_dim] + lstm_out[:, 0, self.hidden_dim:]\n",
        "        else:\n",
        "            # If not bidirectional, just take the last timestep output\n",
        "            final_hidden_state = lstm_out[:, -1, :]\n",
        "\n",
        "        # Apply Dropout, then fully connected layer\n",
        "        output = self.fc(self.dropout(self.relu(final_hidden_state)))\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# Read extracted features for each video\n",
        "def reading_features(hof_path, dist_path, angle_path, ltp_path, velocity_path):\n",
        "    with open(dist_path, 'r') as f:\n",
        "        distance_features = json.load(f)\n",
        "\n",
        "    with open(hof_path, 'r') as f:\n",
        "        hof_features = json.load(f)\n",
        "\n",
        "    with open(angle_path, 'r') as f:\n",
        "        angle_features = json.load(f)\n",
        "\n",
        "    with open(ltp_path, 'r') as f:\n",
        "        ltp_features = json.load(f)\n",
        "\n",
        "    with open(velocity_path, 'r') as f:\n",
        "        vel_features = json.load(f)\n",
        "\n",
        "    # print(\"distance\", np.array(distance_features['intra_inter_distance_mapping']).shape)\n",
        "    # print(\"angle\", np.array(angle_features['intra_inter_angle_mapping']).shape)\n",
        "    # print(\"hof\", np.array(hof_features['hof']).shape)\n",
        "    # print(\"ltp\", np.array(ltp_features['ltp']).shape)\n",
        "    # print(\"vel\", np.array(vel_features['velocity']).shape)\n",
        "\n",
        "    combined_features = np.concatenate((\n",
        "        np.array(distance_features['intra_inter_distance_mapping']),\n",
        "        np.array(angle_features['intra_inter_angle_mapping']),\n",
        "        np.array(vel_features['velocity']),\n",
        "        np.array(ltp_features['ltp']),\n",
        "        np.array(hof_features['hof']),\n",
        "    ), axis=1)\n",
        "\n",
        "    return combined_features\n",
        "\n",
        "\n",
        "def get_samples(X, Y, group_ids):\n",
        "    num_videos, num_frames, num_features = X.shape\n",
        "    num_classes = len(np.unique(Y))\n",
        "    # Step 4: Divide into smaller video samples using a sliding window approach\n",
        "    window_size = 40\n",
        "    step_size = 2\n",
        "    all_video_samples = []\n",
        "\n",
        "    for video_idx in range(num_videos):\n",
        "        video_sample = X[video_idx]\n",
        "        video_samples = [video_sample[start:start + window_size] for start in range(0, num_frames - window_size + 1, step_size)]\n",
        "        all_video_samples.append(video_samples)\n",
        "\n",
        "\n",
        "    X = np.array(all_video_samples)\n",
        "    num_samples_per_video = X.shape[1]\n",
        "    X = X.reshape(-1, window_size, num_features)\n",
        "    Y = np.repeat(Y, num_samples_per_video)\n",
        "    group_ids = np.repeat(group_ids, num_samples_per_video)\n",
        "\n",
        "    return X, Y, group_ids"
      ],
      "metadata": {
        "id": "jFrnsEZ3zJOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Read extracted features for each video\n",
        "def reading_features(hof_path, dist_path, angle_path, ltp_path, velocity_path):\n",
        "\n",
        "    if os.path.exists(dist_path):\n",
        "      with open(dist_path, 'r') as f:\n",
        "          distance_features = json.load(f)\n",
        "      np.save(dist_path.split('.')[0] + '.npy', np.array(distance_features['intra_inter_distance_mapping']))\n",
        "      os.remove(dist_path)\n",
        "\n",
        "    if os.path.exists(hof_path):\n",
        "      with open(hof_path, 'r') as f:\n",
        "          hof_features = json.load(f)\n",
        "      np.save(hof_path.split('.')[0] + '.npy', np.array(hof_features['hof']))\n",
        "      os.remove(hof_path)\n",
        "\n",
        "    if os.path.exists(angle_path):\n",
        "      with open(angle_path, 'r') as f:\n",
        "          angle_features = json.load(f)\n",
        "      np.save(angle_path.split('.')[0] + '.npy', np.array(angle_features['intra_inter_angle_mapping']))\n",
        "      os.remove(angle_path)\n",
        "\n",
        "    if os.path.exists(ltp_path):\n",
        "      with open(ltp_path, 'r') as f:\n",
        "          ltp_features = json.load(f)\n",
        "      np.save(ltp_path.split('.')[0] + '.npy', np.array(ltp_features['ltp']))\n",
        "      os.remove(ltp_path)\n",
        "\n",
        "\n",
        "    if os.path.exists(velocity_path):\n",
        "      with open(velocity_path, 'r') as f:\n",
        "          vel_features = json.load(f)\n",
        "      np.save(velocity_path.split('.')[0] + '.npy', np.array(vel_features['velocity']))\n",
        "      os.remove(velocity_path)\n",
        "\n",
        "\n",
        "    # geo_features = np.load(geo_path)\n",
        "    # print(\"distance\", np.array(intra_inter_distance_features['intra_inter_distance_mapping']).shape)\n",
        "    # print(\"angle\", np.array(angle_features['intra_inter_angle_mapping']).shape)\n",
        "    # print(\"hof\", np.array(hof_features['hof']).shape)\n",
        "    # print(\"ltp\", np.array(ltp_features['ltp']).shape)\n",
        "    # print(\"hog\", np.array(hog_features['hog']).shape)\n",
        "    # print(\"vel\", np.array(vel_features['velocity']).shape)\n",
        "\n",
        "    # combined_features = np.concatenate((\n",
        "    #     np.array(distance_features['distance']),\n",
        "    #     geo_features,\n",
        "    #     # np.array(angle_features['angle']),\n",
        "    #     # np.array(vel_features['velocity']),\n",
        "    #     # np.array(hof_features['hof']),\n",
        "    #     # np.array(hog_features['hog']),\n",
        "\n",
        "    #     np.array(ltp_features['ltp']),\n",
        "    #     np.array(hof_features['hof']),\n",
        "    #     # np.array(lpq_features['lpq'])\n",
        "    # ), axis=1)\n",
        "\n",
        "    # return combined_features\n",
        "#  [\"dist\", \"angle\", \"vel\", \"hog\", \"hof\"]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming you have a method to get your data as a DataFrame\n",
        "    video_df = get_ground_truth()\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    group_ids = []\n",
        "\n",
        "    for video, video_row in video_df.iterrows():\n",
        "        reading_features(video_row['fixed_hof_path'], video_row['dist_path'], video_row['angle_path'],\n",
        "                      video_row['ltp_path'], video_row['vel_path'])\n",
        "        print(\"processed\", video)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db0mefaers8x",
        "outputId": "271540f8-20ab-4a82-f707-d076c53bbad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 1_2\n",
            "processed 2_2\n",
            "processed 3_2\n",
            "processed 4_2\n",
            "processed 16_1\n",
            "processed 15_1\n",
            "processed 14_1\n",
            "processed 13_1\n",
            "processed 12_3\n",
            "processed 11_3\n",
            "processed 9_3\n",
            "processed 7_3\n",
            "processed 17_1\n",
            "processed 18_2\n",
            "processed 19_3\n",
            "processed 20_1\n",
            "processed 23_1\n",
            "processed 26_1\n",
            "processed 29_1\n",
            "processed 32_2\n",
            "processed 33_1\n",
            "processed 34_3\n",
            "processed 35_2\n",
            "processed 36_1\n",
            "processed 37_3\n",
            "processed 38_2\n",
            "processed 40_3\n",
            "processed 39_1\n",
            "processed 41_3\n",
            "processed 43_2\n",
            "processed 42_1\n",
            "processed 44_3\n",
            "processed 45_1\n",
            "processed 46_2\n",
            "processed 22_3\n",
            "processed 21_2\n",
            "processed 24_2\n",
            "processed 25_3\n",
            "processed 27_2\n",
            "processed 28_3\n",
            "processed 30_2\n",
            "processed 31_3\n",
            "processed 47_3\n",
            "processed 48_2\n",
            "processed 49_1\n",
            "processed 50_3\n",
            "processed 51_2\n",
            "processed 52_1\n",
            "processed 53_3\n",
            "processed 54_2\n",
            "processed 55_1\n",
            "processed 57_3\n",
            "processed 56_2\n",
            "processed 58_1\n",
            "processed 59_3\n",
            "processed 60_2\n",
            "processed 61_1\n",
            "processed 62_3\n",
            "processed 63_2\n",
            "processed 64_1\n",
            "processed 65_3\n",
            "processed 66_2\n",
            "processed 67_1\n",
            "processed 68_3\n",
            "processed 69_1\n",
            "processed 71_2\n",
            "processed 72_1\n",
            "processed 73_3\n",
            "processed 74_2\n",
            "processed 76_1\n",
            "processed 75_3\n",
            "processed 77_2\n",
            "processed 77_3\n",
            "processed 78_2\n",
            "processed 79_1\n",
            "processed 80_3\n",
            "processed 81_2\n",
            "processed 82_1\n",
            "processed 83_3\n",
            "processed 85_2\n",
            "processed 84_1\n",
            "processed 86_3\n",
            "processed 87_2\n",
            "processed 88_1\n",
            "processed 89_3\n",
            "processed 90_2\n",
            "processed 91_1\n",
            "processed 92_3\n",
            "processed 93_2\n",
            "processed 94_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# {'hidden_dim': 128, 'num_layers': 1, 'lr': 0.001, 'bidirectional': True}\n",
        "# Best Accuracy: 90.71% (40, 2)\n",
        "\n",
        "# best params hidden_dim=64, num_layers=2, lr=1e-3, epochs=50\n",
        "def classificationWithBiLSTM(X, Y, group_ids, hidden_dim=128, num_layers=1, lr=1e-3, bidirectional=True, epochs=100):\n",
        "    X, Y, group_ids = get_samples(X, Y, group_ids)\n",
        "    num_videos, window_size, num_features = X.shape\n",
        "    print(\"num_features\", num_features)\n",
        "    num_classes = len(np.unique(Y))\n",
        "\n",
        "    all_y_true = []\n",
        "    all_y_pred = []\n",
        "\n",
        "    save_dir = '/content/drive/MyDrive/FYP/DL_training'\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    SCALERS = {\n",
        "      \"dist\": MinMaxScaler(),\n",
        "      \"angle\": MinMaxScaler(),\n",
        "      \"hof\": StandardScaler(),\n",
        "      \"vel\": RobustScaler(),\n",
        "      \"ltp\": MinMaxScaler()\n",
        "    }\n",
        "\n",
        "    selected_features_len = {\n",
        "      \"dist\": 12,\n",
        "      \"angle\": 15,\n",
        "      \"hof\": 18,\n",
        "      \"vel\": 14,\n",
        "      \"ltp\": 512\n",
        "    }\n",
        "\n",
        "    selected_features = [\"dist\", \"angle\", \"vel\", \"ltp\", \"hof\"]\n",
        "    logo = LeaveOneGroupOut()\n",
        "    accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
        "\n",
        "    for train_idx, test_idx in logo.split(X, Y, groups=group_ids):\n",
        "        X_train, X_test = X[train_idx], X[test_idx]\n",
        "        Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
        "\n",
        "        # print(\"X_train shape\", X_train.shape)\n",
        "        # print(\"X_test shape\", X_test.shape)\n",
        "        # print(\"Y_train shape\", Y_train.shape)\n",
        "        # print(\"Y_test shape\", Y_test.shape)\n",
        "\n",
        "        prev_feat_length = 0\n",
        "        standard_X_train, standard_X_test = None, None\n",
        "\n",
        "        for feature in selected_features:\n",
        "            X_train_subset = X_train[:, :, prev_feat_length:prev_feat_length + selected_features_len[feature]].reshape(X_train.shape[0], -1)\n",
        "            X_test_subset = X_test[:, :, prev_feat_length:prev_feat_length + selected_features_len[feature]].reshape(X_test.shape[0], -1)\n",
        "\n",
        "            # X_train_subset = X_train[:, :, prev_feat_length:prev_feat_length + selected_features_len[feature]].reshape(X_train.shape[0], -1)\n",
        "            # X_test_subset = X_test[:, :, prev_feat_length:prev_feat_length + selected_features_len[feature]].reshape(X_test.shape[0], -1)\n",
        "\n",
        "            # print(\"feature\", feature)\n",
        "            # print(\"X_train\", X_train_subset.shape)\n",
        "            # print(\"Y_train\", Y_train.shape)\n",
        "            print(\"feature\", feature)\n",
        "            print(\"X_train_subset shape\", X_test_subset.shape)\n",
        "            print(\"X_test_subset shape\", X_test_subset.shape)\n",
        "            scaler = SCALERS[feature]\n",
        "            X_train_subset = scaler.fit_transform(X_train_subset)\n",
        "            X_test_subset = scaler.transform(X_test_subset)\n",
        "\n",
        "            scalar_model_path = os.path.join(save_dir, f\"scaler_{feature}_fold_{len(accuracies)}.joblib\")\n",
        "            joblib.dump(scaler, scalar_model_path)\n",
        "\n",
        "            X_train_subset = X_train_subset.reshape(X_train.shape[0], window_size, -1)\n",
        "            X_test_subset = X_test_subset.reshape(X_test.shape[0], window_size, -1)\n",
        "\n",
        "            if standard_X_train is None:\n",
        "                standard_X_train = X_train_subset\n",
        "                standard_X_test = X_test_subset\n",
        "            else:\n",
        "                standard_X_train = np.concatenate((standard_X_train, X_train_subset), axis=2)\n",
        "                standard_X_test = np.concatenate((standard_X_test, X_test_subset), axis=2)\n",
        "\n",
        "            prev_feat_length += selected_features_len[feature]\n",
        "\n",
        "        X_train = standard_X_train\n",
        "        X_test = standard_X_test\n",
        "\n",
        "        # Convert data to PyTorch tensors\n",
        "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "        Y_train_tensor = torch.tensor(Y_train, dtype=torch.long).to(device)\n",
        "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "        Y_test_tensor = torch.tensor(Y_test, dtype=torch.long).to(device)\n",
        "\n",
        "        train_data = Data.TensorDataset(X_train_tensor, Y_train_tensor)\n",
        "        test_data = Data.TensorDataset(X_test_tensor, Y_test_tensor)\n",
        "\n",
        "        train_loader = Data.DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
        "        test_loader = Data.DataLoader(dataset=test_data, batch_size=16, shuffle=False)\n",
        "\n",
        "        best_accuracy = 0.0  # Track best accuracy\n",
        "        best_model_path = f\"{save_dir}/best_model_fold_{len(accuracies)}.pth\"  # Unique model filename for each fold\n",
        "\n",
        "        model = BiLSTM(num_features, hidden_dim, num_classes, num_layers, bidirectional).to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "        # Learning Rate Scheduler\n",
        "        # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n",
        "\n",
        "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "        # best_loss = float('inf')\n",
        "        best_accuracy = 0.0  # Track best accuracy\n",
        "        best_model_path = f\"{save_dir}/best_model_fold_{len(accuracies)}.pth\"  # Unique model filename for each fold\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            for inputs, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # print(\"inputs shape\", inputs.shape)\n",
        "                # print(\"labels shape\", labels.shape)\n",
        "                # print(\"outputs shape\", outputs.shape)\n",
        "                # print(\"labels shape\", labels.shape)\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Validation (after each epoch)\n",
        "            model.eval()\n",
        "            y_pred_fold, y_true_fold = [], []\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in test_loader:\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "                    outputs = model(inputs)\n",
        "\n",
        "                    # print(\"outputs shape\", outputs.shape)\n",
        "                    # print(\"labels shape\", labels.shape)\n",
        "\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    y_pred_fold.extend(preds.cpu().numpy())\n",
        "                    y_true_fold.extend(labels.cpu().numpy())\n",
        "            val_loss /= len(test_loader)\n",
        "\n",
        "            # Compute validation accuracy\n",
        "            accuracy = accuracy_score(y_true_fold, y_pred_fold)\n",
        "\n",
        "            if accuracy > best_accuracy:  # Save model if accuracy improves\n",
        "                best_accuracy = accuracy\n",
        "                torch.save(model.state_dict(), best_model_path)\n",
        "                print(f\"Epoch {epoch+1}: Validation Loss: {val_loss}\")\n",
        "\n",
        "            # scheduler.step(val_loss)  # Update learning rate\n",
        "            scheduler.step()\n",
        "\n",
        "        print(f'Best accuracy for fold {len(accuracies)}: {best_accuracy * 100:.2f}% (Model saved)')\n",
        "\n",
        "        # Testing\n",
        "        model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
        "        model.eval()\n",
        "\n",
        "        y_pred_fold, y_true_fold = [], []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                y_pred_fold.extend(preds.detach().cpu().numpy())  # Ensure moving tensors to CPU before converting to numpy\n",
        "                y_true_fold.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        # Metrics\n",
        "        accuracy = accuracy_score(y_true_fold, y_pred_fold)\n",
        "        precision = precision_score(y_true_fold, y_pred_fold, average='macro', zero_division=0)\n",
        "        recall = recall_score(y_true_fold, y_pred_fold, average='macro')\n",
        "        f1 = f1_score(y_true_fold, y_pred_fold, average='macro')\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        all_y_pred.extend(y_pred_fold)\n",
        "        all_y_true.extend(y_true_fold)\n",
        "\n",
        "        print(f'Fold accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    # Confusion matrix visualization\n",
        "    cm = confusion_matrix(all_y_true, all_y_pred, labels=np.unique(Y))\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    class_names = ['doing own work', 'passing paper', \"looking at other's work\"]\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=class_names)\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    disp.plot(ax=ax, cmap=plt.cm.Blues, values_format=\".2f\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f'Mean precision: {np.mean(precisions):.4f}')\n",
        "    print(f'Mean recall: {np.mean(recalls):.4f}')\n",
        "    print(f'Mean BiLSTM accuracy: {np.mean(accuracies) * 100:.2f}%')\n",
        "    print(f\"Mean F1-score: {np.mean(f1_scores)}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming you have a method to get your data as a DataFrame\n",
        "    video_df = get_ground_truth()\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    group_ids = []\n",
        "\n",
        "    for video, video_row in video_df.iterrows():\n",
        "        combined_feat  = reading_features(video_row['hof_path'], video_row['dist_path'], video_row['angle_path'],\n",
        "                      video_row['ltp_path'], video_row['fixed_velocity_path'])\n",
        "\n",
        "        # print(\"combined_features\", combined_feat.shape)\n",
        "        X.append(combined_feat)\n",
        "        Y.append(video_row['action'])\n",
        "        group_ids.append(video_row['fold'])\n",
        "\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    classificationWithBiLSTM(X, Y, group_ids)\n",
        "\n"
      ],
      "metadata": {
        "id": "z75aGCwnUnOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# 32, 64, 128, 256]\n",
        "# Main classification function with grid search\n",
        "def gridSearchWithBiLSTM(X, Y, group_ids, hidden_dims=[128, 256], num_layers_list=[1, 2], lrs=[1e-3], bidirectional_list=[False, True], epochs=100):\n",
        "    X, Y, group_ids = get_samples(X, Y, group_ids)\n",
        "    num_videos, window_size, num_features = X.shape\n",
        "    num_classes = len(np.unique(Y))\n",
        "\n",
        "    overall_best_accuracy = 0\n",
        "    best_params = {}\n",
        "    save_dir = \"/content/drive/MyDrive/FYP/DL_training\"\n",
        "\n",
        "    SCALERS = {\n",
        "      \"dist\": MinMaxScaler(),\n",
        "      \"angle\": MinMaxScaler(),\n",
        "      \"hof\": StandardScaler(),\n",
        "      \"vel\": RobustScaler(),\n",
        "      \"ltp\": MinMaxScaler(),\n",
        "    }\n",
        "\n",
        "    selected_features_len = {\n",
        "      \"dist\": 12,\n",
        "      \"angle\": 15,\n",
        "      \"hof\": 18,\n",
        "      \"vel\": 14,\n",
        "      \"ltp\": 512\n",
        "      # \"efficientNet\": 1408\n",
        "    }\n",
        "\n",
        "    selected_features = [\"dist\", \"angle\", \"vel\", \"ltp\", \"hof\"]\n",
        "\n",
        "    # Loop over all combinations of parameters\n",
        "    for hidden_dim in hidden_dims:\n",
        "        for num_layers in num_layers_list:\n",
        "            for lr in lrs:\n",
        "                for bidirectional in bidirectional_list:\n",
        "\n",
        "                    print(f\"Training with hidden_dim={hidden_dim}, num_layers={num_layers}, lr={lr}, bidirectional={bidirectional}\")\n",
        "\n",
        "                    logo = LeaveOneGroupOut()\n",
        "                    accuracies = []\n",
        "\n",
        "                    for train_idx, test_idx in logo.split(X, Y, groups=group_ids):\n",
        "\n",
        "                        X_train, X_test = X[train_idx], X[test_idx]\n",
        "                        Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
        "\n",
        "                        print(\"X_test shape\", X_test.shape)\n",
        "\n",
        "                        prev_feat_length = 0\n",
        "                        standard_X_train, standard_X_test = None, None\n",
        "\n",
        "                        for feature in selected_features:\n",
        "                            X_train_subset = X_train[:, :, prev_feat_length:prev_feat_length + selected_features_len[feature]].reshape(X_train.shape[0], -1)\n",
        "                            X_test_subset = X_test[:, :, prev_feat_length:prev_feat_length + selected_features_len[feature]].reshape(X_test.shape[0], -1)\n",
        "\n",
        "                            # print(\"feature\", feature)\n",
        "                            # print(\"X_train\", X_train_subset.shape)\n",
        "                            # print(\"Y_train\", Y_train.shape)\n",
        "\n",
        "                            scaler = SCALERS[feature]\n",
        "                            X_train_subset = scaler.fit_transform(X_train_subset)\n",
        "                            X_test_subset = scaler.transform(X_test_subset)\n",
        "\n",
        "                            X_train_subset = X_train_subset.reshape(X_train.shape[0], window_size, -1)\n",
        "                            X_test_subset = X_test_subset.reshape(X_test.shape[0], window_size, -1)\n",
        "\n",
        "                            if standard_X_train is None:\n",
        "                                standard_X_train = X_train_subset\n",
        "                                standard_X_test = X_test_subset\n",
        "                            else:\n",
        "                                standard_X_train = np.concatenate((standard_X_train, X_train_subset), axis=2)\n",
        "                                standard_X_test = np.concatenate((standard_X_test, X_test_subset), axis=2)\n",
        "\n",
        "                            prev_feat_length += selected_features_len[feature]\n",
        "\n",
        "                        X_train = standard_X_train\n",
        "                        X_test = standard_X_test\n",
        "\n",
        "                        print(\"X_train\", X_train.shape)\n",
        "                        print(\"Y_train\", Y_train.shape)\n",
        "\n",
        "                        # Convert data to PyTorch tensors\n",
        "                        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "                        Y_train_tensor = torch.tensor(Y_train, dtype=torch.long).to(device)\n",
        "                        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "                        Y_test_tensor = torch.tensor(Y_test, dtype=torch.long).to(device)\n",
        "\n",
        "                        train_data = Data.TensorDataset(X_train_tensor, Y_train_tensor)\n",
        "                        test_data = Data.TensorDataset(X_test_tensor, Y_test_tensor)\n",
        "\n",
        "                        train_loader = Data.DataLoader(dataset=train_data, batch_size=32, shuffle=True)\n",
        "                        test_loader = Data.DataLoader(dataset=test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "                        if bidirectional:\n",
        "                            epochs = 100\n",
        "                            # lr=0.0001\n",
        "\n",
        "                        model = BiLSTM(num_features, hidden_dim, num_classes, num_layers, bidirectional).to(device)\n",
        "\n",
        "                        criterion = nn.CrossEntropyLoss()\n",
        "                        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "                        # Learning Rate Scheduler\n",
        "                        # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n",
        "\n",
        "                        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "                        # best_loss = float('inf')\n",
        "                        best_accuracy = 0.0  # Track best accuracy\n",
        "                        best_model_path = f\"{save_dir}/best_model_fold_{len(accuracies)}.pth\"  # Unique model filename for each fold\n",
        "\n",
        "                        # Training loop\n",
        "                        for epoch in range(epochs):\n",
        "                            model.train()\n",
        "                            for inputs, labels in train_loader:\n",
        "                                optimizer.zero_grad()\n",
        "                                inputs = inputs.to(device)\n",
        "                                labels = labels.to(device)\n",
        "                                outputs = model(inputs)\n",
        "\n",
        "                                # print(\"inputs shape\", inputs.shape)\n",
        "                                # print(\"labels shape\", labels.shape)\n",
        "                                # print(\"outputs shape\", outputs.shape)\n",
        "                                # print(\"labels shape\", labels.shape)\n",
        "\n",
        "                                loss = criterion(outputs, labels)\n",
        "                                loss.backward()\n",
        "                                optimizer.step()\n",
        "\n",
        "                            # Validation (after each epoch)\n",
        "                            model.eval()\n",
        "                            y_pred_fold, y_true_fold = [], []\n",
        "                            val_loss = 0.0\n",
        "                            with torch.no_grad():\n",
        "                                for inputs, labels in test_loader:\n",
        "                                    inputs = inputs.to(device)\n",
        "                                    labels = labels.to(device)\n",
        "                                    outputs = model(inputs)\n",
        "\n",
        "                                    # print(\"outputs shape\", outputs.shape)\n",
        "                                    # print(\"labels shape\", labels.shape)\n",
        "\n",
        "                                    loss = criterion(outputs, labels)\n",
        "                                    val_loss += loss.item()\n",
        "\n",
        "                                    _, preds = torch.max(outputs, 1)\n",
        "                                    y_pred_fold.extend(preds.cpu().numpy())\n",
        "                                    y_true_fold.extend(labels.cpu().numpy())\n",
        "                            val_loss /= len(test_loader)\n",
        "\n",
        "                            # Compute validation accuracy\n",
        "                            accuracy = accuracy_score(y_true_fold, y_pred_fold)\n",
        "\n",
        "                            if accuracy > best_accuracy:  # Save model if accuracy improves\n",
        "                                best_accuracy = accuracy\n",
        "                                torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "                            # scheduler.step(val_loss)  # Update learning rate\n",
        "                            scheduler.step()\n",
        "                            # print(f\"Epoch {epoch+1}: Validation Loss: {val_loss}\")\n",
        "\n",
        "                        accuracies.append(best_accuracy)\n",
        "\n",
        "                    mean_accuracy = np.mean(accuracies)\n",
        "\n",
        "                    print(\"mean accuracy\", mean_accuracy)\n",
        "\n",
        "                    # Update best parameters if current combination is better\n",
        "                    if mean_accuracy > overall_best_accuracy:\n",
        "                        overall_best_accuracy = mean_accuracy\n",
        "                        best_params = {\n",
        "                            \"hidden_dim\": hidden_dim,\n",
        "                            \"num_layers\": num_layers,\n",
        "                            \"lr\": lr,\n",
        "                            \"bidirectional\": bidirectional\n",
        "                        }\n",
        "\n",
        "    # Output the best parameters\n",
        "    print(\"Best Parameters:\")\n",
        "    print(best_params)\n",
        "    print(f\"Best Accuracy: {overall_best_accuracy * 100:.2f}%\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming you have a method to get your data as a DataFrame\n",
        "    video_df = get_ground_truth()\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    group_ids = []\n",
        "\n",
        "    for video, video_row in video_df.iterrows():\n",
        "        combined_feat  = reading_features(video_row['hof_path'], video_row['dist_path'], video_row['angle_path'],\n",
        "                      video_row['ltp_path'], video_row['fixed_velocity_path'])\n",
        "\n",
        "        # print(\"combined_features\", combined_feat.shape)\n",
        "        X.append(combined_feat)\n",
        "        Y.append(video_row['action'])\n",
        "        group_ids.append(video_row['fold'])\n",
        "\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    gridSearchWithBiLSTM(X, Y, group_ids)\n",
        "\n",
        "# dist (60, 12)\n",
        "# angle (60, 15)\n",
        "# hog (60, 3780)\n",
        "# vel (60, 28)\n",
        "# ltp (60, 512)\n",
        "# hof (60, 18)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb9Zxa7Xnnr8",
        "outputId": "b71dc9a2-7f5f-4e93-9638-168826bec833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with hidden_dim=128, num_layers=1, lr=0.001, bidirectional=False\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "mean accuracy 0.907070707070707\n",
            "Training with hidden_dim=128, num_layers=1, lr=0.001, bidirectional=True\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "mean accuracy 0.8868686868686867\n",
            "Training with hidden_dim=128, num_layers=2, lr=0.001, bidirectional=False\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "mean accuracy 0.8777777777777778\n",
            "Training with hidden_dim=128, num_layers=2, lr=0.001, bidirectional=True\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "mean accuracy 0.8888888888888888\n",
            "Training with hidden_dim=256, num_layers=1, lr=0.001, bidirectional=False\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "mean accuracy 0.8989898989898989\n",
            "Training with hidden_dim=256, num_layers=1, lr=0.001, bidirectional=True\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "mean accuracy 0.9101010101010102\n",
            "Training with hidden_dim=256, num_layers=2, lr=0.001, bidirectional=False\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "mean accuracy 0.8989898989898989\n",
            "Training with hidden_dim=256, num_layers=2, lr=0.001, bidirectional=True\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "X_test shape (330, 40, 571)\n",
            "X_train (660, 40, 571)\n",
            "Y_train (660,)\n",
            "mean accuracy 0.8838383838383838\n",
            "Best Parameters:\n",
            "{'hidden_dim': 256, 'num_layers': 1, 'lr': 0.001, 'bidirectional': True}\n",
            "Best Accuracy: 91.01%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vp_az4k4kwnd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}